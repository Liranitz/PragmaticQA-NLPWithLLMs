{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ['XAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Configure DSPy with an LM FIRST (before creating SemanticF1)\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric (now it will work because LM is configured)\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the test set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"test.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        if doc['qas'] and len(doc['qas']) > 0:\n",
    "            first_qa = doc['qas'][0]\n",
    "            first_questions.append({\n",
    "                'question': first_qa.get('q', ''),  # Use 'q' for question\n",
    "                'answers': [first_qa.get('a', '')],  # Use 'a' for answer, wrap in list for compatibility\n",
    "                'literal_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('literal_obj', [])],\n",
    "                'pragmatic_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('pragmatic_obj', [])]\n",
    "            })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 first questions from the test set.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "first_questions = first_questions[0:10]\n",
    "print(f\"Loaded {len(first_questions)} first questions from the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up retriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files(directory):\n",
    "    from bs4 import BeautifulSoup\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "corpus = read_html_files(\"../PragmatiCQA-sources/The Legend of Zelda\")\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean version without batch evaluation attempts\n",
    "def evaluate_qa_system_clean(questions, context_type='retrieved'):\n",
    "    \"\"\"Evaluate QA system with different context configurations using SemanticF1.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answers'][0]  # Using first answer as reference\n",
    "        \n",
    "        # Get context based on configuration\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q['literal_spans'])\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q['pragmatic_spans'])\n",
    "        else:  # retrieved\n",
    "            context = ' '.join(search(question).passages)\n",
    "        \n",
    "        # Get prediction from QA model\n",
    "        if context.strip():\n",
    "            prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "        \n",
    "        examples.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'context': context\n",
    "        })\n",
    "    \n",
    "    # Create metric and evaluate individually\n",
    "    metric = SemanticF1()\n",
    "    scores = []\n",
    "    \n",
    "    for i, ex in enumerate(examples):\n",
    "        try:\n",
    "            print(f\"Evaluating question {i+1}/{len(examples)}...\")\n",
    "            gold_example = dspy.Example(question=ex['question'], response=ex['reference'])\n",
    "            pred_example = dspy.Example(question=ex['question'], response=ex['prediction'])\n",
    "            score = metric(gold_example, pred_example)\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            scores.append(0)\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        results.append({\n",
    "            'question': ex['question'],\n",
    "            'prediction': ex['prediction'],\n",
    "            'reference': ex['reference'],\n",
    "            'score': scores[i] if i < len(scores) else 0\n",
    "        })\n",
    "    \n",
    "    avg_score = sum(r['score'] for r in results) / len(results) if results else 0\n",
    "    return results, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating literal configuration...\n",
      "Average SemanticF1 Score: 0.4485\n",
      "\n",
      "Evaluating pragmatic configuration...\n",
      "Average SemanticF1 Score: 0.4638\n",
      "\n",
      "Evaluating retrieved configuration...\n",
      "Average SemanticF1 Score: 0.2133\n"
     ]
    }
   ],
   "source": [
    "# Use the clean function\n",
    "configurations = ['literal', 'pragmatic', 'retrieved']\n",
    "clean_results = {}\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nEvaluating {config} configuration...\")\n",
    "    eval_results, avg_score = evaluate_qa_system_clean(first_questions, config)\n",
    "    clean_results[config] = {\n",
    "        'avg_score': avg_score,\n",
    "        'detailed_results': eval_results\n",
    "    }\n",
    "    print(f\"Average SemanticF1 Score: {avg_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPREHENSIVE EVALUATION RESULTS\n",
      "============================================================\n",
      "Configuration   | SemanticF1 Score\n",
      "-----------------------------------\n",
      "literal         |          0.4485\n",
      "pragmatic       |          0.4638\n",
      "retrieved       |          0.2133\n"
     ]
    }
   ],
   "source": [
    "# Print tabular report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"{'Configuration':<15} | {'SemanticF1 Score':>15}\")\n",
    "print(\"-\" * 35)\n",
    "for config, result in clean_results.items():\n",
    "    print(f\"{config:<15} | {result['avg_score']:>15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for literal configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: What year did the Legend of Zelda come out?\n",
      "Prediction: 1986\n",
      "Reference: The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\n",
      "Score: 0.6667\n",
      "\n",
      "Worst performing example:\n",
      "Question: What kind of game is The Legend of Zelda?\n",
      "Prediction: Zelda\n",
      "Reference: The Legend of Zelda is one that includes roleplaying, action, adventure, and puzzle/logic. It is the first installment of the Zelda series and centers its plot around a boy named Link.\n",
      "Score: 0.0000\n",
      "\n",
      "Analysis for pragmatic configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: What year did the Legend of Zelda come out?\n",
      "Prediction: 1986\n",
      "Reference: The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\n",
      "Score: 0.6667\n",
      "\n",
      "Worst performing example:\n",
      "Question: What system did the game first come out in? \n",
      "Prediction: It has since then been re-released several times\n",
      "Reference: It was first released on NES (the Nintendo Entertainment System) in 1987, and has since been released on many different systems created by Nintendo.\n",
      "Score: 0.0000\n",
      "\n",
      "Analysis for retrieved configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: What year did the Legend of Zelda come out?\n",
      "Prediction: 1986\n",
      "Reference: The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\n",
      "Score: 0.6667\n",
      "\n",
      "Worst performing example:\n",
      "Question: when did the legend of zelda last until?\n",
      "Prediction: June 19, 2011\n",
      "Reference: The Legend of Zelda is the first installment in the Zelda franchise, and its success allowed the development of sequels with nearly every title in the series influenced by this one. The latest installment of the series was with Nintendo Switch Online, released last on April 23, 2019.\n",
      "Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Analysis of results\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze where the model succeeds and fails.\"\"\"\n",
    "    for config in results:\n",
    "        print(f\"\\nAnalysis for {config} configuration:\")\n",
    "        scores = [r['score'] for r in results[config]['detailed_results']]\n",
    "        \n",
    "        # Get best and worst performing examples\n",
    "        best_idx = scores.index(max(scores))\n",
    "        worst_idx = scores.index(min(scores))\n",
    "        \n",
    "        print(\"\\nBest performing example:\")\n",
    "        best_example = results[config]['detailed_results'][best_idx]\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Score: {best_example['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\nWorst performing example:\")\n",
    "        worst_example = results[config]['detailed_results'][worst_idx]\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Score: {worst_example['score']:.4f}\")\n",
    "\n",
    "analyze_results(clean_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
