{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03933a822bae4838955e8ae18b1bbacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liran\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59aa66aca6b94037aa100e297fcd7bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41db41d7a0df42f3b36ef6c67e3eb8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b3337046574ccda676c2f8f16bfee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210b4f7043f94c3f83d87ec41b70f59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the test set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"test.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        if doc['qas'] and len(doc['qas']) > 0:\n",
    "            first_qa = doc['qas'][0]\n",
    "            first_questions.append({\n",
    "                'question': first_qa.get('q', ''),  # Use 'q' for question\n",
    "                'answers': [first_qa.get('a', '')],  # Use 'a' for answer, wrap in list for compatibility\n",
    "                'literal_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('literal_obj', [])],\n",
    "                'pragmatic_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('pragmatic_obj', [])]\n",
    "            })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 213 first questions from the test set.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "\n",
    "print(f\"Loaded {len(first_questions)} first questions from the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfc67b2010144d081555b85d7e01185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liran\\.cache\\huggingface\\hub\\models--sentence-transformers--static-retrieval-mrl-en-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f72c83a27f4a969d51cf79f6213800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8da049f4cd4d23b8f4ded71a4334c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea4bf1c0d648de97e52a466cd5a2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67959abac9774770af7fe23c81764dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/125M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up retriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files(directory):\n",
    "    from bs4 import BeautifulSoup\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "corpus = read_html_files(\"../PragmatiCQA-sources/The Legend of Zelda\")\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_system(questions, context_type='retrieved'):\n",
    "    \"\"\"Evaluate QA system with different context configurations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answers'][0]  # Using first answer as reference\n",
    "        \n",
    "        # Get context based on configuration\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q['literal_spans'])\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q['pragmatic_spans'])\n",
    "        else:  # retrieved\n",
    "            context = ' '.join(search(question).passages)\n",
    "        \n",
    "        # Get prediction from QA model\n",
    "        if context.strip():\n",
    "            prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score = metric(prediction=prediction, reference=reference)\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_score = sum(r['score'] for r in results) / len(results)\n",
    "    return results, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating literal configuration...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SemanticF1.forward() got an unexpected keyword argument 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configurations:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m configuration...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     eval_results, avg_score = \u001b[43mevaluate_qa_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     results[config] = {\n\u001b[32m      9\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mavg_score\u001b[39m\u001b[33m'\u001b[39m: avg_score,\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdetailed_results\u001b[39m\u001b[33m'\u001b[39m: eval_results\n\u001b[32m     11\u001b[39m     }\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage SemanticF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mevaluate_qa_system\u001b[39m\u001b[34m(questions, context_type)\u001b[39m\n\u001b[32m     21\u001b[39m         prediction = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     score = \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     results.append({\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m: question,\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m: prediction,\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m: reference,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m: score\n\u001b[32m     30\u001b[39m     })\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Calculate average scores\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\utils\\callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\nlp-with-llms-2025-hw3\\.venv\\Lib\\site-packages\\dspy\\primitives\\program.py:60\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: SemanticF1.forward() got an unexpected keyword argument 'prediction'"
     ]
    }
   ],
   "source": [
    "# Run evaluations for all three configurations\n",
    "configurations = ['literal', 'pragmatic', 'retrieved']\n",
    "results = {}\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nEvaluating {config} configuration...\")\n",
    "    eval_results, avg_score = evaluate_qa_system(first_questions, config)\n",
    "    results[config] = {\n",
    "        'avg_score': avg_score,\n",
    "        'detailed_results': eval_results\n",
    "    }\n",
    "    print(f\"Average SemanticF1 Score: {avg_score:.4f}\")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nComparison of configurations:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Configuration':<15} | {'SemanticF1 Score':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for config in configurations:\n",
    "    print(f\"{config:<15} | {results[config]['avg_score']:>15.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of results\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze where the model succeeds and fails.\"\"\"\n",
    "    for config in results:\n",
    "        print(f\"\\nAnalysis for {config} configuration:\")\n",
    "        scores = [r['score'] for r in results[config]['detailed_results']]\n",
    "        \n",
    "        # Get best and worst performing examples\n",
    "        best_idx = scores.index(max(scores))\n",
    "        worst_idx = scores.index(min(scores))\n",
    "        \n",
    "        print(\"\\nBest performing example:\")\n",
    "        best_example = results[config]['detailed_results'][best_idx]\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Score: {best_example['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\nWorst performing example:\")\n",
    "        worst_example = results[config]['detailed_results'][worst_idx]\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Score: {worst_example['score']:.4f}\")\n",
    "\n",
    "analyze_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
