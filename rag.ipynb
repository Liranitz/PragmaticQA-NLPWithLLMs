{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ['XAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Configure DSPy with an LM FIRST (before creating SemanticF1)\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric (now it will work because LM is configured)\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the test set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"test.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        if doc['qas'] and len(doc['qas']) > 0:\n",
    "            first_qa = doc['qas'][0]\n",
    "            first_questions.append({\n",
    "                'question': first_qa.get('q', ''),  # Use 'q' for question\n",
    "                'answer': first_qa.get('a', ''),  # Use 'a' for answer\n",
    "                'literal_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('literal_obj', [])],\n",
    "                'pragmatic_spans': [obj['text'] for obj in first_qa.get('a_meta', {}).get('pragmatic_obj', [])]\n",
    "            })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 213 first questions from the test set.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "print(f\"Loaded {len(first_questions)} first questions from the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up retriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files(directory):\n",
    "    from bs4 import BeautifulSoup\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "corpus = read_html_files(\"../PragmatiCQA-sources/The Legend of Zelda\")\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean version without batch evaluation attempts\n",
    "def evaluate_qa_system_clean(questions, context_type='retrieved'):\n",
    "    \"\"\"Evaluate QA system with different context configurations using SemanticF1.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answer']  # Using first answer as reference\n",
    "        \n",
    "        # Get context based on configuration\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q['literal_spans'])\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q['pragmatic_spans'])\n",
    "        else:  # retrieved\n",
    "            context = ' '.join(search(question).passages)\n",
    "        \n",
    "        # Get prediction from QA model\n",
    "        if context.strip():\n",
    "            prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "        \n",
    "        examples.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'context': context\n",
    "        })\n",
    "    \n",
    "    # Create metric and evaluate individually\n",
    "    metric = SemanticF1()\n",
    "    scores = []\n",
    "    \n",
    "    for i, ex in enumerate(examples):\n",
    "        try:\n",
    "            print(f\"Evaluating question {i+1}/{len(examples)}...\")\n",
    "            gold_example = dspy.Example(question=ex['question'], response=ex['reference'])\n",
    "            pred_example = dspy.Example(question=ex['question'], response=ex['prediction'])\n",
    "            score = metric(gold_example, pred_example)\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            scores.append(0)\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        results.append({\n",
    "            'question': ex['question'],\n",
    "            'prediction': ex['prediction'],\n",
    "            'reference': ex['reference'],\n",
    "            'score': scores[i] if i < len(scores) else 0\n",
    "        })\n",
    "    \n",
    "    avg_score = sum(r['score'] for r in results) / len(results) if results else 0\n",
    "    return results, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_system_clean_batched(questions, context_type='retrieved', batch_size=5):\n",
    "    \"\"\"Evaluate QA system with simple batching for better performance.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answer']\n",
    "        \n",
    "        # Get context based on configuration\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q['literal_spans'])\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q['pragmatic_spans'])\n",
    "        else:  # retrieved\n",
    "            context = ' '.join(search(question).passages)\n",
    "        \n",
    "        # Get prediction from QA model\n",
    "        if context.strip():\n",
    "            prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "        \n",
    "        examples.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'context': context\n",
    "        })\n",
    "    \n",
    "    # Create metric and evaluate in batches\n",
    "    metric = SemanticF1()\n",
    "    scores = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(examples) + batch_size - 1)//batch_size}...\")\n",
    "        \n",
    "        for ex in batch:\n",
    "            try:\n",
    "                gold_example = dspy.Example(question=ex['question'], response=ex['reference'])\n",
    "                pred_example = dspy.Example(question=ex['question'], response=ex['prediction'])\n",
    "                score = metric(gold_example, pred_example)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation failed: {e}\")\n",
    "                scores.append(0)\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        results.append({\n",
    "            'question': ex['question'],\n",
    "            'prediction': ex['prediction'],\n",
    "            'reference': ex['reference'],\n",
    "            'score': scores[i] if i < len(scores) else 0\n",
    "        })\n",
    "    \n",
    "    avg_score = sum(r['score'] for r in results) / len(results) if results else 0\n",
    "    return results, avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating literal configuration...\n",
      "Processing batch 1/43...\n",
      "Processing batch 2/43...\n",
      "Processing batch 3/43...\n",
      "Processing batch 4/43...\n",
      "Processing batch 5/43...\n",
      "Processing batch 6/43...\n",
      "Processing batch 7/43...\n",
      "Processing batch 8/43...\n",
      "Processing batch 9/43...\n",
      "Processing batch 10/43...\n",
      "Processing batch 11/43...\n",
      "Processing batch 12/43...\n",
      "Processing batch 13/43...\n",
      "Processing batch 14/43...\n",
      "Processing batch 15/43...\n",
      "Processing batch 16/43...\n",
      "Processing batch 17/43...\n",
      "Processing batch 18/43...\n",
      "Processing batch 19/43...\n",
      "Processing batch 20/43...\n",
      "Processing batch 21/43...\n",
      "Processing batch 22/43...\n",
      "Processing batch 23/43...\n",
      "Processing batch 24/43...\n",
      "Processing batch 25/43...\n",
      "Processing batch 26/43...\n",
      "Processing batch 27/43...\n",
      "Processing batch 28/43...\n",
      "Processing batch 29/43...\n",
      "Processing batch 30/43...\n",
      "Processing batch 31/43...\n",
      "Processing batch 32/43...\n",
      "Processing batch 33/43...\n",
      "Processing batch 34/43...\n",
      "Processing batch 35/43...\n",
      "Processing batch 36/43...\n",
      "Processing batch 37/43...\n",
      "Processing batch 38/43...\n",
      "Processing batch 39/43...\n",
      "Processing batch 40/43...\n",
      "Processing batch 41/43...\n",
      "Processing batch 42/43...\n",
      "Processing batch 43/43...\n",
      "Average SemanticF1 Score: 0.4311\n",
      "\n",
      "Evaluating pragmatic configuration...\n",
      "Processing batch 1/43...\n",
      "Processing batch 2/43...\n",
      "Processing batch 3/43...\n",
      "Processing batch 4/43...\n",
      "Processing batch 5/43...\n",
      "Processing batch 6/43...\n",
      "Processing batch 7/43...\n",
      "Processing batch 8/43...\n",
      "Processing batch 9/43...\n",
      "Processing batch 10/43...\n",
      "Processing batch 11/43...\n",
      "Processing batch 12/43...\n",
      "Processing batch 13/43...\n",
      "Processing batch 14/43...\n",
      "Processing batch 15/43...\n",
      "Processing batch 16/43...\n",
      "Processing batch 17/43...\n",
      "Processing batch 18/43...\n",
      "Processing batch 19/43...\n",
      "Processing batch 20/43...\n",
      "Processing batch 21/43...\n",
      "Processing batch 22/43...\n",
      "Processing batch 23/43...\n",
      "Processing batch 24/43...\n",
      "Processing batch 25/43...\n",
      "Processing batch 26/43...\n",
      "Processing batch 27/43...\n",
      "Processing batch 28/43...\n",
      "Processing batch 29/43...\n",
      "Processing batch 30/43...\n",
      "Processing batch 31/43...\n",
      "Processing batch 32/43...\n",
      "Processing batch 33/43...\n",
      "Processing batch 34/43...\n",
      "Processing batch 35/43...\n",
      "Processing batch 36/43...\n",
      "Processing batch 37/43...\n",
      "Processing batch 38/43...\n",
      "Processing batch 39/43...\n",
      "Processing batch 40/43...\n",
      "Processing batch 41/43...\n",
      "Processing batch 42/43...\n",
      "Processing batch 43/43...\n",
      "Average SemanticF1 Score: 0.3764\n",
      "\n",
      "Evaluating retrieved configuration...\n",
      "Processing batch 1/43...\n",
      "Processing batch 2/43...\n",
      "Processing batch 3/43...\n",
      "Processing batch 4/43...\n",
      "Processing batch 5/43...\n",
      "Processing batch 6/43...\n",
      "Processing batch 7/43...\n",
      "Processing batch 8/43...\n",
      "Processing batch 9/43...\n",
      "Processing batch 10/43...\n",
      "Processing batch 11/43...\n",
      "Processing batch 12/43...\n",
      "Processing batch 13/43...\n",
      "Processing batch 14/43...\n",
      "Processing batch 15/43...\n",
      "Processing batch 16/43...\n",
      "Processing batch 17/43...\n",
      "Processing batch 18/43...\n",
      "Processing batch 19/43...\n",
      "Processing batch 20/43...\n",
      "Processing batch 21/43...\n",
      "Processing batch 22/43...\n",
      "Processing batch 23/43...\n",
      "Processing batch 24/43...\n",
      "Processing batch 25/43...\n",
      "Processing batch 26/43...\n",
      "Processing batch 27/43...\n",
      "Processing batch 28/43...\n",
      "Processing batch 29/43...\n",
      "Processing batch 30/43...\n",
      "Processing batch 31/43...\n",
      "Processing batch 32/43...\n",
      "Processing batch 33/43...\n",
      "Processing batch 34/43...\n",
      "Processing batch 35/43...\n",
      "Processing batch 36/43...\n",
      "Processing batch 37/43...\n",
      "Processing batch 38/43...\n",
      "Processing batch 39/43...\n",
      "Processing batch 40/43...\n",
      "Processing batch 41/43...\n",
      "Processing batch 42/43...\n",
      "Processing batch 43/43...\n",
      "Average SemanticF1 Score: 0.0247\n"
     ]
    }
   ],
   "source": [
    "# Use the clean function\n",
    "configurations = ['literal', 'pragmatic', 'retrieved']\n",
    "clean_results = {}\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nEvaluating {config} configuration...\")\n",
    "    eval_results, avg_score = evaluate_qa_system_clean_batched(first_questions, config)\n",
    "    clean_results[config] = {\n",
    "        'avg_score': avg_score,\n",
    "        'detailed_results': eval_results\n",
    "    }\n",
    "    print(f\"Average SemanticF1 Score: {avg_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to clean_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save clean_results to a file after evaluation\n",
    "with open(\"clean_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_results, f, ensure_ascii=False, indent=2)\n",
    "print(\"Results saved to clean_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPREHENSIVE EVALUATION RESULTS\n",
      "============================================================\n",
      "Configuration   | SemanticF1 Score\n",
      "-----------------------------------\n",
      "literal         |          0.4311\n",
      "pragmatic       |          0.3764\n",
      "retrieved       |          0.0247\n"
     ]
    }
   ],
   "source": [
    "# Print tabular report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"{'Configuration':<15} | {'SemanticF1 Score':>15}\")\n",
    "print(\"-\" * 35)\n",
    "for config, result in clean_results.items():\n",
    "    print(f\"{config:<15} | {result['avg_score']:>15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for literal configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: Will LEGO have any new themes for 2023?\n",
      "Prediction: I don't know\n",
      "Reference: I don't know\n",
      "Score: 1.0000\n",
      "\n",
      "Worst performing example:\n",
      "Question: What kind of game is The Legend of Zelda?\n",
      "Prediction: Zelda\n",
      "Reference: The Legend of Zelda is one that includes roleplaying, action, adventure, and puzzle/logic. It is the first installment of the Zelda series and centers its plot around a boy named Link.\n",
      "Score: 0.0000\n",
      "\n",
      "Analysis for pragmatic configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: What year was Mystery Science Theater 3000 release?\n",
      "Prediction: 1988\n",
      "Reference: Good question.  The show was picked up and debuted in November 1988 with two episodes airing back-to-back. Joel served as host, now named \" Joel Robinson \". Hide Selected Literal Answer Spans The show was picked up and debuted in November 1988 with two episodes airing back-to-back. Joel served as host, now named \" Joel Robinson \".  Hide Added Additional Info Since the show's humble UHF beginnings in 1988, it has gained a dedicated fanbase that remains to this day.\n",
      "Score: 1.0000\n",
      "\n",
      "Worst performing example:\n",
      "Question: What system did the game first come out in? \n",
      "Prediction: It has since then been re-released several times\n",
      "Reference: It was first released on NES (the Nintendo Entertainment System) in 1987, and has since been released on many different systems created by Nintendo.\n",
      "Score: 0.0000\n",
      "\n",
      "Analysis for retrieved configuration:\n",
      "\n",
      "Best performing example:\n",
      "Question: What year did the Legend of Zelda come out?\n",
      "Prediction: 1986\n",
      "Reference: The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\n",
      "Score: 0.6667\n",
      "\n",
      "Worst performing example:\n",
      "Question: when did the legend of zelda last until?\n",
      "Prediction: June 19, 2011\n",
      "Reference: The Legend of Zelda is the first installment in the Zelda franchise, and its success allowed the development of sequels with nearly every title in the series influenced by this one. The latest installment of the series was with Nintendo Switch Online, released last on April 23, 2019.\n",
      "Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Analysis of results\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze where the model succeeds and fails.\"\"\"\n",
    "    for config in results:\n",
    "        print(f\"\\nAnalysis for {config} configuration:\")\n",
    "        scores = [r['score'] for r in results[config]['detailed_results']]\n",
    "        \n",
    "        # Get best and worst performing examples\n",
    "        best_idx = scores.index(max(scores))\n",
    "        worst_idx = scores.index(min(scores))\n",
    "        \n",
    "        print(\"\\nBest performing example:\")\n",
    "        best_example = results[config]['detailed_results'][best_idx]\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Score: {best_example['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\nWorst performing example:\")\n",
    "        worst_example = results[config]['detailed_results'][worst_idx]\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Score: {worst_example['score']:.4f}\")\n",
    "\n",
    "analyze_results(clean_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
