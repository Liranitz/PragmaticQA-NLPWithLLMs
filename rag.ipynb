{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the test set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"test.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        if doc['qas'] and len(doc['qas']) > 0:\n",
    "            first_questions.append({\n",
    "                'question': doc['qas'][0]['question'],\n",
    "                'answers': doc['qas'][0]['answers'],\n",
    "                'literal_spans': doc['qas'][0].get('literal_spans', []),\n",
    "                'pragmatic_spans': doc['qas'][0].get('pragmatic_spans', [])\n",
    "            })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "print(f\"Loaded {len(first_questions)} first questions from the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up retriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files(directory):\n",
    "    from bs4 import BeautifulSoup\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "corpus = read_html_files(\"../PragmatiCQA-sources/The Legend of Zelda\")\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_qa_system(questions, context_type='retrieved'):\n",
    "    \"\"\"Evaluate QA system with different context configurations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answers'][0]  # Using first answer as reference\n",
    "        \n",
    "        # Get context based on configuration\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q['literal_spans'])\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q['pragmatic_spans'])\n",
    "        else:  # retrieved\n",
    "            context = ' '.join(search(question).passages)\n",
    "        \n",
    "        # Get prediction from QA model\n",
    "        if context.strip():\n",
    "            prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score = metric(prediction=prediction, reference=reference)\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_score = sum(r['score'] for r in results) / len(results)\n",
    "    return results, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluations for all three configurations\n",
    "configurations = ['literal', 'pragmatic', 'retrieved']\n",
    "results = {}\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nEvaluating {config} configuration...\")\n",
    "    eval_results, avg_score = evaluate_qa_system(first_questions, config)\n",
    "    results[config] = {\n",
    "        'avg_score': avg_score,\n",
    "        'detailed_results': eval_results\n",
    "    }\n",
    "    print(f\"Average SemanticF1 Score: {avg_score:.4f}\")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nComparison of configurations:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Configuration':<15} | {'SemanticF1 Score':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for config in configurations:\n",
    "    print(f\"{config:<15} | {results[config]['avg_score']:>15.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analysis of results\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze where the model succeeds and fails.\"\"\"\n",
    "    for config in results:\n",
    "        print(f\"\\nAnalysis for {config} configuration:\")\n",
    "        scores = [r['score'] for r in results[config]['detailed_results']]\n",
    "        \n",
    "        # Get best and worst performing examples\n",
    "        best_idx = scores.index(max(scores))\n",
    "        worst_idx = scores.index(min(scores))\n",
    "        \n",
    "        print(\"\\nBest performing example:\")\n",
    "        best_example = results[config]['detailed_results'][best_idx]\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Score: {best_example['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\nWorst performing example:\")\n",
    "        worst_example = results[config]['detailed_results'][worst_idx]\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Score: {worst_example['score']:.4f}\")\n",
    "\n",
    "analyze_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
