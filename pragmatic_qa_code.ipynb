{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 - Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Part 1: The \"Traditional\" NLP Approach\n",
    "In this part, you will use a pre-trained Question Answering model as a \"traditional\" baseline.\n",
    "\n",
    "Use the retriever module from the rag.ipynb notebook to retrieve relevant passage given a question. Concatenate all the elements into a single context.\n",
    "\n",
    "Write a program that uses a pre-trained QA model from Hugging Face's transformers library to generate an answer to the question given the context retrieved by the retriever module. A good model to use for this is 'distilbert-base-cased-distilled-squad' (see https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad). This module will extract an answer from the context without explicit multi-step reasoning.\n",
    "\n",
    "Evaluate on PRAGMATICQA: Run your model on the PRAGMATICQA test set. Since the answers are free-form, you need to use a metric like F1 score or ROUGE to evaluate performance. In this assignment, we will use the SemanticF1 metric provided in DSPy which uses an LLM as a Judge method.\n",
    "\n",
    "Since the performance of the model depends on the accuracy of the retriever module, we will compute three different configurations:\n",
    "\n",
    "Literal answer: the answer generated by the distilbert model from the literal spans included in the dataset.\n",
    "Pragmatic answer: the answer generated by the distilbert model from the pragmatic spans included in the dataset.\n",
    "Retrieved answer: the answer generated by the distilbert model from the context computed by the retriever.\n",
    "For each of these three configurations, report precision, recall and F1 as computed by SemanticF1 on the validation dataset for the first question of each conversation only (there are 179 such cases in val.jsonl). For the first question in each conversation, there is no \"conversational context\", hence the input to the model only includes the question and the retrieved passages.\n",
    "\n",
    "Note: To improve performance, consider using the SemanticF1.batch method to perform the dataset evaluation (https://dspy.ai/api/evaluation/SemanticF1/) or the general dspy.Evaluate evaluation method which runs evaluation in parallel and outputs a tabular report.\n",
    "\n",
    "Analyze the results: where does the model succeed, and where does it fail? Does it tend to give literal answers when a more pragmatic one is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ['XAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Configure DSPy with an LM FIRST (before creating SemanticF1)\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric (now it will work because LM is configured)\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and extract questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the val set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"val.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation - keep topic too.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        qas = doc.get('qas', [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        first_qa = qas[0]\n",
    "        first_questions.append({\n",
    "            'topic': doc.get('topic', ''),  # needed to choose the right retriever\n",
    "            'question': first_qa.get('q', ''),\n",
    "            'answer': first_qa.get('a', ''),\n",
    "            # support both possible field names\n",
    "            'literal_spans': [obj.get('text','') for obj in first_qa.get('a_meta', {}).get('literal_obj', [])] or first_qa.get('literal_spans', []),\n",
    "            'pragmatic_spans': [obj.get('text','') for obj in first_qa.get('a_meta', {}).get('pragmatic_obj', [])] or first_qa.get('pragmatic_spans', [])\n",
    "        })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 first questions from the val set.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "print(f\"Loaded {len(first_questions)} first questions from the val set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded aliases -> existing folder names\n",
    "TOPIC_ALIASES = {\n",
    "    \"A Nightmare on Elm Street (2010 film)\": \"A Nightmare on Elm Street\",\n",
    "    \"Alexander Hamilton\": \"Hamilton the Musical\",      # proxy topic\n",
    "    \"Popeye\": \"Popeye the Sailor\",\n",
    "    \"The Wonderful Wizard of Oz (book)\": \"Wizard of Oz\",\n",
    "}\n",
    "\n",
    "import os\n",
    "\n",
    "def list_available_topics(sources_dir):\n",
    "    return {\n",
    "        name for name in os.listdir(sources_dir)\n",
    "        if os.path.isdir(os.path.join(sources_dir, name))\n",
    "    }\n",
    "\n",
    "def resolve_topic_name(topic, available_topics):\n",
    "    \"\"\"Return a folder topic to use for this logical topic.\"\"\"\n",
    "    # exact match first\n",
    "    if topic in available_topics:\n",
    "        return topic\n",
    "    # alias match next\n",
    "    alias = TOPIC_ALIASES.get(topic)\n",
    "    if alias and alias in available_topics:\n",
    "        return alias\n",
    "    # nothing found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built retrievers for 11/11 topics.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# P2.2 - Load HTML sources and build per-topic retrievers\n",
    "\n",
    "def create_retriever_for_topic(topic, sources_dir=\"../PragmatiCQA-sources\", k=5):\n",
    "    topic_dir = os.path.join(sources_dir, topic) if topic else None\n",
    "    corpus = read_html_files_with_stopper(topic_dir) if topic_dir else []\n",
    "    if not corpus:\n",
    "        return None\n",
    "    return dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=k)\n",
    "\n",
    "def cache_topic_retrievers(split_data, sources_dir=\"../PragmatiCQA-sources\", k=5):\n",
    "    topics = {ex.get(\"topic\",\"\") for ex in split_data if ex.get(\"topic\",\"\")}\n",
    "    available = list_available_topics(sources_dir)\n",
    "    cache = {}\n",
    "    missing = []\n",
    "\n",
    "    for t in sorted(topics):\n",
    "        folder_topic = resolve_topic_name(t, available) or t\n",
    "        r = create_retriever_for_topic(folder_topic, sources_dir=sources_dir, k=k)\n",
    "        if r is not None:\n",
    "            # keep key as the ORIGINAL topic from the dataset,\n",
    "            # but build the retriever from the resolved folder topic\n",
    "            cache[t] = r\n",
    "        else:\n",
    "            missing.append(t)\n",
    "\n",
    "    print(f\"Built retrievers for {len(cache)}/{len(topics)} topics.\")\n",
    "    if missing:\n",
    "        print(\"Missing or empty topic folders:\", missing)\n",
    "    return cache\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files_with_stopper(directory):\n",
    "    docs = []\n",
    "    if not os.path.isdir(directory):\n",
    "        return docs\n",
    "    for fn in os.listdir(directory):\n",
    "        if fn.endswith(\".html\"):\n",
    "            p = os.path.join(directory, fn)\n",
    "            try:\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "                    text = soup.get_text(separator=\" \", strip=True)\n",
    "                    if text:\n",
    "                        # clamp to avoid huge prompts\n",
    "                        docs.append(text[:3000])\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {fn}: {e}\")\n",
    "    return docs\n",
    "\n",
    "# Build per-topic retrievers from the topics present in this split\n",
    "retr_lookup_val = cache_topic_retrievers(test_data, sources_dir=\"../PragmatiCQA-sources\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics missing retrievers (4): ['A Nightmare on Elm Street (2010 film)', 'Alexander Hamilton', 'Popeye', 'The Wonderful Wizard of Oz (book)']\n"
     ]
    }
   ],
   "source": [
    "topics_in_data = {ex.get(\"topic\",\"\") for ex in test_data if ex.get(\"topic\",\"\")}\n",
    "topics_in_cache = set(retr_lookup_val.keys())\n",
    "missing = topics_in_data - topics_in_cache\n",
    "print(f\"Topics missing retrievers ({len(missing)}): {sorted(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 73 topic folders under:\n",
      "C:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\PragmatiCQA-sources\n",
      "\n",
      " - 'Cats' Musical\n",
      " - A Nightmare on Elm Street\n",
      " - Arrowverse\n",
      " - Barney\n",
      " - Baseball\n",
      " - Batman\n",
      " - Big Nate\n",
      " - Bleach\n",
      " - Britney Spears\n",
      " - Detective Conan\n",
      " - Dinosaur\n",
      " - Doctor Who\n",
      " - Doom Patrol\n",
      " - Dr. Stone\n",
      " - Dream Team\n",
      " - Edens Zero\n",
      " - Enter the Gungeon\n",
      " - Evangelion\n",
      " - Fallout\n",
      " - Fullmetal Alchemist\n",
      " - Game of Thrones\n",
      " - Girl Genius\n",
      " - Goosebumps\n",
      " - H. P. Lovecraft\n",
      " - Half-Life series\n",
      " - Halo\n",
      " - Hamilton the Musical\n",
      " - Harry Potter\n",
      " - Inazuma Eleven\n",
      " - Indiana Jones\n",
      " - Jujutsu Kaisen\n",
      " - Kingdom Hearts\n",
      " - Kung Fu Panda\n",
      " - LEGO\n",
      " - Lady Gaga\n",
      " - Lemony Snicket\n",
      " - MS Paint Adventures\n",
      " - Madagascar\n",
      " - My Hero Academia\n",
      " - Mystery Science Theater 3000\n",
      " - Non-alien Creatures\n",
      " - Olympics\n",
      " - One Piece\n",
      " - Peanuts Comics\n",
      " - Pixar\n",
      " - Popeye the Sailor\n",
      " - Rap\n",
      " - Reborn\n",
      " - Riordan\n",
      " - Serious Sam\n",
      " - Shaman King\n",
      " - ShowBiz Pizza\n",
      " - Six the Musical\n",
      " - Skulduggery Pleasant\n",
      " - Sonic the Hedgehog\n",
      " - Soul Eater\n",
      " - Stargate\n",
      " - Studio Ghibli\n",
      " - Supernanny\n",
      " - Taylor Swift\n",
      " - The BIONICLE\n",
      " - The Dark Crystal\n",
      " - The Formula 1\n",
      " - The Grisha universe\n",
      " - The Karate Kid\n",
      " - The Legend of Zelda\n",
      " - The Matrix\n",
      " - The Maze Runner\n",
      " - The Wheel of Time\n",
      " - Throne of Glass\n",
      " - Umbrella Academy\n",
      " - Wings of Fire\n",
      " - Wizard of Oz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sources_root = r\"C:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\PragmatiCQA-sources\"\n",
    "\n",
    "if not os.path.exists(sources_root):\n",
    "    print(f\"‚ùå Path not found: {sources_root}\")\n",
    "else:\n",
    "    folders = [name for name in os.listdir(sources_root)\n",
    "               if os.path.isdir(os.path.join(sources_root, name))]\n",
    "    print(f\"üìÅ Found {len(folders)} topic folders under:\\n{sources_root}\\n\")\n",
    "    for f in sorted(folders):\n",
    "        print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method with keeping the full report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETR_TOPK = 3\n",
    "CTX_MAX_CHARS = 2000\n",
    "\n",
    "def to_strings(passages):\n",
    "    out = []\n",
    "    for p in passages:\n",
    "        if isinstance(p, str):\n",
    "            out.append(p)\n",
    "        elif hasattr(p, \"text\"):\n",
    "            out.append(str(p.text))\n",
    "        else:\n",
    "            out.append(str(p))\n",
    "    return out\n",
    "\n",
    "def get_retrieved_context(question, retr):\n",
    "    \"\"\"Return joined context string from a DSPy retriever result, robust to shapes.\"\"\"\n",
    "    try:\n",
    "        res = retr(question)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) list[str] already\n",
    "    if isinstance(res, list):\n",
    "        passages = res\n",
    "\n",
    "    # 2) has attribute .passages\n",
    "    elif hasattr(res, \"passages\"):\n",
    "        passages = res.passages\n",
    "\n",
    "    # 3) dict with 'passages'\n",
    "    elif isinstance(res, dict) and \"passages\" in res:\n",
    "        passages = res[\"passages\"]\n",
    "\n",
    "    # 4) fallback - stringify\n",
    "    else:\n",
    "        passages = [str(res)]\n",
    "\n",
    "    strings = to_strings(passages)[:RETR_TOPK]\n",
    "    ctx = \" \".join(s.strip() for s in strings if s and s.strip())\n",
    "    return ctx[:CTX_MAX_CHARS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "def _token_prf(prediction, reference):\n",
    "    pt = set(str(prediction).lower().split())\n",
    "    rt = set(str(reference).lower().split())\n",
    "    if not pt or not rt:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    overlap = pt & rt\n",
    "    precision = len(overlap) / len(pt) if pt else 0.0\n",
    "    recall    = len(overlap) / len(rt) if rt else 0.0\n",
    "    f1 = 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_qa_system_full_metrics(\n",
    "    questions,\n",
    "    context_type='retrieved',\n",
    "    batch_size=16,\n",
    "    retr_lookup=None,\n",
    "    qa_pipeline=None,\n",
    "    metric: SemanticF1=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates with:\n",
    "    - DSPy SemanticF1 using dspy.Example (LLM-as-judge)\n",
    "    - token precision, recall, F1\n",
    "    Returns: (detailed_results, avg_precision, avg_recall, avg_f1, avg_semantic_f1)\n",
    "    \"\"\"\n",
    "    if context_type == 'retrieved' and not isinstance(retr_lookup, dict):\n",
    "        raise ValueError(\"context_type='retrieved' requires retr_lookup=dict.\")\n",
    "    if qa_pipeline is None:\n",
    "        raise ValueError(\"qa_pipeline must be provided.\")\n",
    "    if metric is None:\n",
    "        # make sure you did dspy.configure(lm=...) before this\n",
    "        metric = SemanticF1()\n",
    "\n",
    "    examples = []\n",
    "    empty_ctx = 0\n",
    "\n",
    "    # build contexts and predictions\n",
    "    for q in questions:\n",
    "        question  = q['question']\n",
    "        reference = q['answer']\n",
    "\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q.get('literal_spans', [])).strip()\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q.get('pragmatic_spans', [])).strip()\n",
    "        elif context_type == 'retrieved':\n",
    "            topic = q.get('topic', '')\n",
    "            retr = retr_lookup.get(topic) if retr_lookup else None\n",
    "            context = get_retrieved_context(q['question'], retr) if retr else ''\n",
    "\n",
    "        if not context:\n",
    "            empty_ctx += 1\n",
    "            pred = \"\"\n",
    "        else:\n",
    "            try:\n",
    "                out = qa_pipeline(question=question, context=context)\n",
    "                pred = (out.get('answer', '') if isinstance(out, dict) else \"\").strip()\n",
    "            except Exception:\n",
    "                pred = \"\"\n",
    "\n",
    "        examples.append({\n",
    "            'topic': q.get('topic',''),\n",
    "            'question': question,\n",
    "            'prediction': pred,\n",
    "            'reference': reference,\n",
    "            'context_type': context_type,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "    if verbose:\n",
    "        n = len(examples)\n",
    "        print(f\"Built {n} examples - empty contexts: {empty_ctx}/{n} ({empty_ctx/max(1,n):.1%})\")\n",
    "\n",
    "    if not examples:\n",
    "        return [], 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # per-example SemanticF1 using dspy.Example\n",
    "    detailed_results = []\n",
    "    pr_list, re_list, f1_list, sf1_list = [], [], [], []\n",
    "\n",
    "    total = len(examples)\n",
    "    for i in range(0, total, batch_size):\n",
    "        if verbose:\n",
    "            print(f\"scoring batch {i//batch_size+1}/{(total+batch_size-1)//batch_size}\")\n",
    "        batch = examples[i:i+batch_size]\n",
    "        for ex in batch:\n",
    "            pred = ex['prediction']\n",
    "            ref  = ex['reference']\n",
    "\n",
    "            # semantic-F1 via dspy.Example\n",
    "            try:\n",
    "                gold_ex = dspy.Example(question=ex['question'], response=ref)\n",
    "                pred_ex = dspy.Example(question=ex['question'], response=pred)\n",
    "                s_f1 = float(metric(gold_ex, pred_ex))\n",
    "            except Exception:\n",
    "                s_f1 = 0.0\n",
    "\n",
    "            # token PRF\n",
    "            p, r, f = _token_prf(pred, ref)\n",
    "\n",
    "            detailed_results.append({\n",
    "                **ex,\n",
    "                'scores': {\n",
    "                    'precision': p,\n",
    "                    'recall': r,\n",
    "                    'f1': f,\n",
    "                    'semantic_f1': s_f1\n",
    "                }\n",
    "            })\n",
    "            pr_list.append(p); re_list.append(r); f1_list.append(f); sf1_list.append(s_f1)\n",
    "\n",
    "    avg_p   = float(np.mean(pr_list)) if pr_list else 0.0\n",
    "    avg_r   = float(np.mean(re_list)) if re_list else 0.0\n",
    "    avg_f   = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    avg_sf1 = float(np.mean(sf1_list)) if sf1_list else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Mean PRF: P={avg_p:.3f} R={avg_r:.3f} F1={avg_f:.3f} | SemanticF1={avg_sf1:.3f}\")\n",
    "\n",
    "    return detailed_results, avg_p, avg_r, avg_f, avg_sf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating literal...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.522 R=0.074 F1=0.118 | SemanticF1=0.424\n",
      "\n",
      "Evaluating pragmatic...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.563 R=0.091 F1=0.143 | SemanticF1=0.383\n",
      "\n",
      "Evaluating retrieved...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.230 R=0.039 F1=0.062 | SemanticF1=0.149\n"
     ]
    }
   ],
   "source": [
    "# make sure LM is configured before creating the metric\n",
    "# lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "# dspy.configure(lm=lm)\n",
    "\n",
    "metric = SemanticF1()  # created after configure\n",
    "\n",
    "clean_results = {}\n",
    "for cfg in ['literal','pragmatic','retrieved']:\n",
    "    print(f\"\\nEvaluating {cfg}...\")\n",
    "    kwargs = {'retr_lookup': retr_lookup_val} if cfg == 'retrieved' else {}\n",
    "    detailed, avg_p, avg_r, avg_f, avg_sf1 = evaluate_qa_system_full_metrics(\n",
    "        first_questions,\n",
    "        context_type=cfg,\n",
    "        batch_size=32,\n",
    "        retr_lookup=kwargs.get('retr_lookup'),\n",
    "        qa_pipeline=qa_pipeline,\n",
    "        metric=metric,\n",
    "        verbose=True\n",
    "    )\n",
    "    clean_results[cfg] = {\n",
    "        'avg_precision': avg_p,\n",
    "        'avg_recall': avg_r,\n",
    "        'avg_f1': avg_f,\n",
    "        'avg_semantic_f1': avg_sf1,\n",
    "        'detailed_results': detailed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to clean_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "with open(\"clean_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_results, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved to clean_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Overall Evaluation Report\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>semantic_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Literal</th>\n",
       "      <td>0.522104</td>\n",
       "      <td>0.073627</td>\n",
       "      <td>0.118310</td>\n",
       "      <td>0.424383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pragmatic</th>\n",
       "      <td>0.563388</td>\n",
       "      <td>0.090601</td>\n",
       "      <td>0.143291</td>\n",
       "      <td>0.383401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrieved</th>\n",
       "      <td>0.230119</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.149340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           precision    recall        f1  semantic_f1\n",
       "config                                               \n",
       "Literal     0.522104  0.073627  0.118310     0.424383\n",
       "Pragmatic   0.563388  0.090601  0.143291     0.383401\n",
       "Retrieved   0.230119  0.038806  0.062257     0.149340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_num(x, default=0.0):\n",
    "    try:\n",
    "        return float(x) if not (x is None or (isinstance(x, float) and math.isnan(x))) else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def report_overall_metrics(results):\n",
    "    \"\"\"Prints and returns a small dataframe with avg precision/recall/F1/semantic-F1 per configuration.\"\"\"\n",
    "    rows = []\n",
    "    for config, res in results.items():\n",
    "        rows.append({\n",
    "            \"config\": config.capitalize(),\n",
    "            \"precision\": _safe_num(res.get(\"avg_precision\")),\n",
    "            \"recall\": _safe_num(res.get(\"avg_recall\")),\n",
    "            \"f1\": _safe_num(res.get(\"avg_f1\")),\n",
    "            \"semantic_f1\": _safe_num(res.get(\"avg_semantic_f1\")),\n",
    "        })\n",
    "    df = pd.DataFrame(rows).set_index(\"config\").sort_index()\n",
    "    print(\"\\nüìä Overall Evaluation Report\\n\" + \"=\"*40)\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "overall_df = report_overall_metrics(clean_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, metric_key='semantic_f1', topk=1):\n",
    "    \"\"\"\n",
    "    Prints best and worst examples per config by a chosen metric.\n",
    "    metric_key in {'semantic_f1','f1','precision','recall'}.\n",
    "    \"\"\"\n",
    "    for config, bundle in results.items():\n",
    "        detailed = bundle.get('detailed_results', [])\n",
    "        if not detailed:\n",
    "            print(f\"\\n‚ö†Ô∏è No detailed results for configuration '{config}'\")\n",
    "            continue\n",
    "\n",
    "        # collect (score, idx)\n",
    "        scored = []\n",
    "        for i, ex in enumerate(detailed):\n",
    "            val = ex.get('scores', {}).get(metric_key, 0.0)\n",
    "            try:\n",
    "                val = float(val)\n",
    "            except Exception:\n",
    "                val = 0.0\n",
    "            if math.isnan(val):\n",
    "                val = 0.0\n",
    "            scored.append((val, i))\n",
    "\n",
    "        if not scored:\n",
    "            print(f\"\\n‚ö†Ô∏è No '{metric_key}' scores found for '{config}'\")\n",
    "            continue\n",
    "\n",
    "        scored.sort(key=lambda t: t[0], reverse=True)\n",
    "        best = scored[:topk]\n",
    "        worst = list(reversed(scored))[:topk]\n",
    "\n",
    "        print(f\"\\n{'='*60}\\nüîπ Analysis for '{config}' configuration ({metric_key.upper()})\\n{'='*60}\")\n",
    "\n",
    "        print(\"\\n‚úÖ Best example(s):\")\n",
    "        for val, idx in best:\n",
    "            ex = detailed[idx]\n",
    "            print(f\"- Score: {val:.4f}\")\n",
    "            print(f\"  Q: {ex['question']}\")\n",
    "            print(f\"  Pred: {ex['prediction']}\")\n",
    "            print(f\"  Ref: {ex['reference']}\")\n",
    "            print(f\"  Ctx: {ex['context'][:200]}...\\n\")\n",
    "\n",
    "        print(\"‚ùå Worst example(s):\")\n",
    "        for val, idx in worst:\n",
    "            ex = detailed[idx]\n",
    "            print(f\"- Score: {val:.4f}\")\n",
    "            print(f\"  Q: {ex['question']}\")\n",
    "            print(f\"  Pred: {ex['prediction']}\")\n",
    "            print(f\"  Ref: {ex['reference']}\")\n",
    "            print(f\"  Ctx: {ex['context'][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, metric_key='semantic_f1'):\n",
    "    \"\"\"\n",
    "    Analyze where the model succeeds and fails across configurations.\n",
    "    Args:\n",
    "        results (dict): output of evaluate_qa_system_full_metrics() runs.\n",
    "        metric_key (str): which metric to analyze (e.g., 'semantic_f1', 'f1', 'precision', 'recall')\n",
    "    \"\"\"\n",
    "    for config in results:\n",
    "        detailed = results[config].get('detailed_results', [])\n",
    "        if not detailed:\n",
    "            print(f\"\\n‚ö†Ô∏è No detailed results for configuration '{config}'\")\n",
    "            continue\n",
    "\n",
    "        # Extract chosen metric for each example\n",
    "        scores = [ex['scores'].get(metric_key, 0.0) for ex in detailed]\n",
    "\n",
    "        if not scores:\n",
    "            print(f\"\\n‚ö†Ô∏è No '{metric_key}' scores found for '{config}'\")\n",
    "            continue\n",
    "\n",
    "        best_idx = int(max(range(len(scores)), key=lambda i: scores[i]))\n",
    "        worst_idx = int(min(range(len(scores)), key=lambda i: scores[i]))\n",
    "\n",
    "        best_example = detailed[best_idx]\n",
    "        worst_example = detailed[worst_idx]\n",
    "\n",
    "        print(f\"\\n{'='*60}\\nüîπ Analysis for '{config}' configuration ({metric_key.upper()})\\n{'='*60}\")\n",
    "\n",
    "        print(\"\\n‚úÖ Best performing example:\")\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Context snippet: {best_example['context'][:150]}...\")\n",
    "        print(f\"{metric_key}: {best_example['scores'][metric_key]:.4f}\")\n",
    "\n",
    "        print(\"\\n‚ùå Worst performing example:\")\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Context snippet: {worst_example['context'][:150]}...\")\n",
    "        print(f\"{metric_key}: {worst_example['scores'][metric_key]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results (success/failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîπ Analysis for 'literal' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "‚úÖ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: what year was the show release ? \n",
      "  Pred: 2005\n",
      "  Ref: Good morning!The first American Supernanny show began airing on ABC on January 7, 2005.\n",
      "  Ctx: The first American Supernanny show began airing on ABC on January 7, 2005....\n",
      "\n",
      "‚ùå Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: How many books have been published in the Game of Thrones series so far?\n",
      "  Pred: Yes\n",
      "  Ref: There are 5 books in the series and 3 prequel novellas set in the same world.\n",
      "  Ctx: Yes...\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîπ Analysis for 'pragmatic' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "‚úÖ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: Ok, Where does the Supernanny mainly live (country)?\n",
      "  Pred: British\n",
      "  Ref: Supernanny lives in the UK. It is originally a British TV series.\n",
      "  Ctx: a British TV series...\n",
      "\n",
      "‚ùå Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: What is Game of thrones its real or not?\n",
      "  Pred: George R.R. Martin\n",
      "  Ref: It is based on the novel series A Song of Ice and Fire\n",
      "  Ctx: written by George R.R. Martin...\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîπ Analysis for 'retrieved' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "‚úÖ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: How many books are in the Game of Thrones series?\n",
      "  Pred: five\n",
      "  Ref: Game of Thrones, the HBO series is based on the five books of George R. R. Martin's Song of Fire and Ice series.\n",
      "  Ctx: A Song of Ice and Fire is an award-winning series of best-selling books of epic fantasy novels by American author and scriptwriter George R.R. Martin . The series currently comprises five published no...\n",
      "\n",
      "‚ùå Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: What is Game of thrones its real or not?\n",
      "  Pred: The Iron Throne\n",
      "  Ref: It is based on the novel series A Song of Ice and Fire\n",
      "  Ctx: Iron Throne Season(s) 1\n",
      "      ,\n",
      "      2\n",
      "      ,\n",
      "      3\n",
      "      ,\n",
      "      4\n",
      "      ,\n",
      "      5\n",
      "      ,\n",
      "      6\n",
      "      ,\n",
      "      7\n",
      "      ,\n",
      "      8 Status Destroyed (melted by Drogon ) Type Throne\n",
      "      Ruling in...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check if clean_results exists, otherwise load from file\n",
    "if 'clean_results' not in globals() or not clean_results:\n",
    "    if os.path.exists(\"clean_results.json\"):\n",
    "        with open(\"clean_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            clean_results = json.load(f)\n",
    "        print(\"Loaded clean_results from clean_results.json\")\n",
    "    else:\n",
    "        raise RuntimeError(\"clean_results is not initialized and clean_results.json not found.\")\n",
    "\n",
    "analyze_results(clean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results: where does the model succeed, and where does it fail? Does it tend to give literal answers when a more pragmatic one is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 - Part 2: The LLM Multi-Step Prompting Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. Part 2: The LLM Multi-Step Prompting Approach\n",
    "Now, you will build a more sophisticated model using an LLM with multi-step prompting.\n",
    "\n",
    "We will now evaluate all the questions in the conversations, not only the first question of each conversation as in 4.3.\n",
    "\n",
    "In each turn, the model will have access as input to the following:\n",
    "\n",
    "The previous turns as pairs (question, answer)\n",
    "The current question.\n",
    "The context retrieved by the retriever model given the current question.\n",
    "Your model can enrich this by computing additional intermediary fields, for example:\n",
    "\n",
    "A summary of the student's goal or interests based on the conversation history\n",
    "A pragmatic or cooperative need underlying the student's current question (based on the past conversation and retrieved spans)\n",
    "A generated \"cooperative\" question which can be used to re-query the source documents and extract additional context\n",
    "A reasoning field computed by a Chain-of-Thought module for any of these intermediary steps\n",
    "Implement the DSPy Module: Create a DSPy module that uses the strategy you devise to generate a cooperative answer.\n",
    "\n",
    "For reference, you can start from the DSPy tutorials demonstrating variations around RAG:\n",
    "\n",
    "https://dspy.ai/tutorials/rag/ (using a retriever based on FAISS and passage embeddings)\n",
    "https://dspy.ai/tutorials/multihop_search/ (using a retriever based on BM25s)\n",
    "https://dspy.ai/tutorials/agents/ (using a ReACT module with a ColbertV2 retriever)\n",
    "4.4.1 First Questions\n",
    "Perform the same evaluation as in 4.3 on the first questions in each conversation and compare the results of your model with the one in 4.3 based on the traditional text-to-text transformer.\n",
    "\n",
    "4.4.2 Conversational Context\n",
    "Now consider all questions in the conversations and take into account the conversational context.\n",
    "\n",
    "Compile and Evaluate: Compile your DSPy program (you can use a small training set from the PRAGMATICQA data for this) and evaluate it on the validation set using the same metrics as in 4.3. Explain which metric you use to drive the optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.3 - Turn builders\n",
    "def get_all_questions(split_data):\n",
    "    out = []\n",
    "    for conv in split_data:\n",
    "        topic = conv.get(\"topic\",\"\")\n",
    "        history = []\n",
    "        for qa in conv.get(\"qas\", []):\n",
    "            q = qa[\"q\"]; a = qa[\"a\"]\n",
    "            out.append({\n",
    "                \"topic\": topic,\n",
    "                \"question\": q,\n",
    "                \"answers\": [a],\n",
    "                \"conversation_history\": list(history)   # copy current history\n",
    "            })\n",
    "            history.append((q, a))  # update rolling history\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.4 - Multi-step DSPy program - OLD\n",
    "class ConversationAnalyzer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyze = dspy.ChainOfThought(\n",
    "            \"conversation_history, current_question -> user_interests, conversation_goal\"\n",
    "        )\n",
    "    def forward(self, conversation_history, current_question):\n",
    "        return self.analyze(conversation_history=conversation_history,\n",
    "                            current_question=current_question)\n",
    "\n",
    "class PragmaticReasoner(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reason = dspy.ChainOfThought(\n",
    "            \"conversation_history, current_question, retrieved_context -> pragmatic_needs, cooperative_query\"\n",
    "        )\n",
    "    def forward(self, conversation_history, current_question, retrieved_context):\n",
    "        return self.reason(conversation_history=conversation_history,\n",
    "                           current_question=current_question,\n",
    "                           retrieved_context=retrieved_context)\n",
    "\n",
    "class CooperativeAnswerGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(\n",
    "            \"question, user_interests, pragmatic_needs, literal_context, pragmatic_context -> response\"\n",
    "        )\n",
    "    def forward(self, question, user_interests, pragmatic_needs, literal_context, pragmatic_context):\n",
    "        return self.generate(question=question,\n",
    "                             user_interests=user_interests,\n",
    "                             pragmatic_needs=pragmatic_needs,\n",
    "                             literal_context=literal_context,\n",
    "                             pragmatic_context=pragmatic_context)\n",
    "\n",
    "class PragmaticRAG(dspy.Module):\n",
    "    def __init__(self, retriever_lookup, default_k=5, max_context_chars=12000):\n",
    "        super().__init__()\n",
    "        self.retriever_lookup = retriever_lookup\n",
    "        self.default_k = default_k\n",
    "        self.max_context_chars = max_context_chars\n",
    "        self.conv = ConversationAnalyzer()\n",
    "        self.reason = PragmaticReasoner()\n",
    "        self.answerer = CooperativeAnswerGenerator()\n",
    "\n",
    "    def _retrieve(self, topic, query, k=None):\n",
    "        retr = self.retriever_lookup.get(topic)\n",
    "        if retr is None or not query:\n",
    "            return []\n",
    "        retr.k = k or self.default_k\n",
    "        try:\n",
    "            res = retr(query)\n",
    "            passages = list(res.passages) if hasattr(res, \"passages\") else []\n",
    "            return passages\n",
    "        except Exception as e:\n",
    "            print(\"Retrieval error:\", e)\n",
    "            return []\n",
    "\n",
    "    def _join_ctx(self, chunks):\n",
    "        if not chunks:\n",
    "            return \"\"\n",
    "        text = \" \".join(chunks)\n",
    "        # clamp to keep prompts reasonable\n",
    "        return text[: self.max_context_chars]\n",
    "\n",
    "    def forward(self, topic, conversation_history, question, k=None):\n",
    "        # 1) base retrieval on the literal question\n",
    "        base_ctx_chunks = self._retrieve(topic, question, k=k)\n",
    "        base_ctx = self._join_ctx(base_ctx_chunks)\n",
    "\n",
    "        # 2) analyze conversation\n",
    "        conv_out = self.conv(conversation_history=conversation_history,\n",
    "                             current_question=question)\n",
    "        user_interests = getattr(conv_out, \"user_interests\", \"\")\n",
    "        conv_goal = getattr(conv_out, \"conversation_goal\", \"\")\n",
    "\n",
    "        # 3) reason about pragmatic needs and propose a cooperative query\n",
    "        reasoning = self.reason(conversation_history=conversation_history,\n",
    "                                current_question=question,\n",
    "                                retrieved_context=base_ctx)\n",
    "        pragmatic_needs = getattr(reasoning, \"pragmatic_needs\", \"\")\n",
    "        coop_query = getattr(reasoning, \"cooperative_query\", None) or question\n",
    "\n",
    "        # 4) cooperative re-retrieval\n",
    "        coop_ctx_chunks = self._retrieve(topic, coop_query, k=k)\n",
    "        coop_ctx = self._join_ctx(coop_ctx_chunks)\n",
    "\n",
    "        # 5) synthesize cooperative answer\n",
    "        gen = self.answerer(\n",
    "            question=question,\n",
    "            user_interests=user_interests,\n",
    "            pragmatic_needs=pragmatic_needs,\n",
    "            literal_context=base_ctx,\n",
    "            pragmatic_context=coop_ctx\n",
    "        )\n",
    "        final_text = getattr(gen, \"response\", \"\") or \"\"\n",
    "\n",
    "        # Important for Evaluate: return an object with field `response`\n",
    "        return dspy.Prediction(response=final_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P2.5 - Load PRAGMATICQA data splits\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "DATASET_DIR = \"../PragmatiCQA/data\"  # adjust if needed\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, \"train.jsonl\")\n",
    "\n",
    "# optional tiny train slice for compilation\n",
    "train_data = read_jsonl(TRAIN_PATH)[:50]  # small subset to keep compile fast\n",
    "\n",
    "len(test_data), len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.6 - SemanticF1 evaluation wrapper\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "def evaluate_semantic_f1(examples, decompositional=True):\n",
    "    metric = SemanticF1(decompositional=decompositional)\n",
    "    preds, golds = [], []\n",
    "    for ex in examples:\n",
    "        preds.append(dspy.Example(question=ex[\"question\"], response=ex[\"prediction\"], inputs={\"context\": \"\"}))\n",
    "        golds.append(dspy.Example(question=ex[\"question\"], response=ex[\"reference\"], inputs={\"context\": \"\"}))\n",
    "    batch = metric.batch(preds, golds)\n",
    "\n",
    "    detailed = []\n",
    "    P, R, F = [], [], []\n",
    "    for ex, sc in zip(examples, batch):\n",
    "        p = sc.get(\"precision\", 0.0); r = sc.get(\"recall\", 0.0); f = sc.get(\"f1\", 0.0)\n",
    "        detailed.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"prediction\": ex[\"prediction\"],\n",
    "            \"reference\": ex[\"reference\"],\n",
    "            \"topic\": ex.get(\"topic\",\"\"),\n",
    "            \"precision\": p, \"recall\": r, \"score\": f\n",
    "        })\n",
    "        P.append(p); R.append(r); F.append(f)\n",
    "\n",
    "    n = max(1, len(F))\n",
    "    overall = {\"precision\": sum(P)/n, \"recall\": sum(R)/n, \"f1\": sum(F)/n}\n",
    "    return {\"overall\": overall, \"detailed_results\": detailed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/14 11:46:56 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/14 11:46:56 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/14 12:04:54 INFO dspy.evaluate.evaluate: Average Metric: 58.35448739312303 / 179 (32.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1 (First questions) SemanticF1: 32.6\n"
     ]
    }
   ],
   "source": [
    "# 4.4.1 - Build devset for first questions (no history)\n",
    "from dspy.evaluate import Evaluate, SemanticF1\n",
    "\n",
    "def build_devset_first_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        qas = conv.get(\"qas\", [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        q0 = qas[0]\n",
    "        dev.append(\n",
    "            dspy.Example(\n",
    "                topic=conv.get(\"topic\",\"\"),\n",
    "                conversation_history=[],          # no history for first turn\n",
    "                question=q0[\"q\"],\n",
    "                response=q0[\"a\"]                  # gold reference\n",
    "            ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "        )\n",
    "    return dev\n",
    "\n",
    "# cache retrievers once\n",
    "prog_first = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_first = build_devset_first_questions(test_data)\n",
    "evaluator_first = Evaluate(devset=devset_first, metric=SemanticF1(decompositional=True))\n",
    "report_first = evaluator_first(prog_first)\n",
    "print(\"4.4.1 (First questions) SemanticF1:\", report_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built retrievers for 7/11 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 19:14:41 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 19:14:41 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 19:26:24 INFO dspy.evaluate.evaluate: Average Metric: 59.187601294131746 / 179 (33.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1 (First questions) SemanticF1: 33.07\n"
     ]
    }
   ],
   "source": [
    "# 4.4.1 - Build devset for first questions (no history)\n",
    "from dspy.evaluate import Evaluate, SemanticF1\n",
    "\n",
    "def build_devset_first_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        qas = conv.get(\"qas\", [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        q0 = qas[0]\n",
    "        dev.append(\n",
    "            dspy.Example(\n",
    "                topic=conv.get(\"topic\",\"\"),\n",
    "                conversation_history=[],          # no history for first turn\n",
    "                question=q0[\"q\"],\n",
    "                response=q0[\"a\"]                  # gold reference\n",
    "            ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "        )\n",
    "    return dev\n",
    "\n",
    "# cache retrievers once\n",
    "prog_first = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_first = build_devset_first_questions(test_data)\n",
    "evaluator_first = Evaluate(devset=devset_first, metric=SemanticF1(decompositional=True))\n",
    "report_first = evaluator_first(prog_first)\n",
    "print(\"4.4.1 (First questions) SemanticF1:\", report_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1 - Build devset for first questions (no history)\n",
    "from dspy.evaluate import Evaluate, SemanticF1\n",
    "\n",
    "def build_devset_first_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        qas = conv.get(\"qas\", [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        q0 = qas[0]\n",
    "        dev.append(\n",
    "            dspy.Example(\n",
    "                topic=conv.get(\"topic\",\"\"),\n",
    "                conversation_history=[],          # no history for first turn\n",
    "                question=q0[\"q\"],\n",
    "                response=q0[\"a\"]                  # gold reference\n",
    "            ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "        )\n",
    "    return dev\n",
    "\n",
    "# cache retrievers once\n",
    "prog_first = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_first = build_devset_first_questions(test_data)\n",
    "evaluator_first = Evaluate(devset=devset_first, metric=SemanticF1(decompositional=True))\n",
    "report_first = evaluator_first(prog_first)\n",
    "print(\"4.4.1 (First questions) SemanticF1:\", report_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built retrievers for 7/11 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/12 20:47:33 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 20:47:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 20:49:03 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 20:49:03 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:08:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:08:19 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:09:01 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:09:01 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:09:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:09:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:37:56 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:38:14 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:38:14 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:38:28 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:38:28 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 21:49:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 21:49:43 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:03:14 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:03:14 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:03:52 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:03:52 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:13:59 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:13:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:14:33 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:14:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:14:38 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:14:38 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:14:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:14:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:15:07 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/12 22:15:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/12 22:15:28 INFO dspy.evaluate.evaluate: Average Metric: 471.5585260847488 / 1526 (30.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.2 (All turns) SemanticF1: 30.9\n"
     ]
    }
   ],
   "source": [
    "# 4.4.2 - Build devset for all questions with rolling history\n",
    "def build_devset_all_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        topic = conv.get(\"topic\",\"\")\n",
    "        hist = []\n",
    "        for qa in conv.get(\"qas\", []):\n",
    "            dev.append(\n",
    "                dspy.Example(\n",
    "                    topic=topic,\n",
    "                    conversation_history=list(hist),   # copy history so far\n",
    "                    question=qa[\"q\"],\n",
    "                    response=qa[\"a\"]\n",
    "                ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "            )\n",
    "            hist.append((qa[\"q\"], qa[\"a\"]))  # update rolling history\n",
    "    return dev\n",
    "\n",
    "retr_lookup_val = cache_topic_retrievers(test_data, k=5)\n",
    "prog_all = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_all = build_devset_all_questions(test_data)\n",
    "evaluator_all = Evaluate(devset=devset_all, metric=SemanticF1(decompositional=True))\n",
    "report_all = evaluator_all(prog_all)\n",
    "print(\"4.4.2 (All turns) SemanticF1:\", report_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to part2_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save all evaluation results to JSON ---\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": \"PragmaticRAG\",\n",
    "    \"dataset\": \"PragmatiCQA\",\n",
    "    \"results\": {\n",
    "        \"part_4_4_1_first_questions\": {\n",
    "            \"semantic_f1\": str(report_first)\n",
    "        },\n",
    "        \"part_4_4_2_all_turns\": {\n",
    "            \"semantic_f1\": str(report_all)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "OUTPUT_FILE = \"part2_results.json\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import numpy as np\n",
    "\n",
    "def _token_prf(prediction, reference):\n",
    "    pt = set(str(prediction).lower().split())\n",
    "    rt = set(str(reference).lower().split())\n",
    "    if not pt or not rt:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    overlap = pt & rt\n",
    "    precision = len(overlap) / len(pt) if pt else 0.0\n",
    "    recall = len(overlap) / len(rt) if rt else 0.0\n",
    "    f1 = 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_qa_system_full_metrics(questions, context_type='retrieved', batch_size=5):\n",
    "    \"\"\"\n",
    "    Build predictions, then evaluate per example:\n",
    "    - semantic_f1 from DSPy SemanticF1 (float)\n",
    "    - token precision, recall, f1 as a simple overlap baseline\n",
    "    Returns: (detailed_results, avg_precision, avg_recall, avg_f1, avg_semantic_f1)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "\n",
    "    # 1) Build examples with context and predictions\n",
    "    for q in questions:\n",
    "        question = q['question']\n",
    "        reference = q['answer']\n",
    "\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q.get('literal_spans', []))\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q.get('pragmatic_spans', []))\n",
    "        else:\n",
    "            # If you use a retriever, make sure `search` exists. Otherwise default to empty.\n",
    "            try:\n",
    "                context = ' '.join(search(question).passages)  # replace if you have a different retriever\n",
    "            except Exception:\n",
    "                context = ''\n",
    "\n",
    "        if context.strip():\n",
    "            try:\n",
    "                prediction = qa_pipeline(question=question, context=context)['answer']\n",
    "            except Exception:\n",
    "                prediction = \"\"\n",
    "        else:\n",
    "            prediction = \"\"\n",
    "\n",
    "        examples.append({\n",
    "            'question': question,\n",
    "            'prediction': prediction,\n",
    "            'reference': reference,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "    # 2) Evaluate\n",
    "    metric = dspy.evaluate.SemanticF1()\n",
    "    detailed_results = []\n",
    "    pr_list, re_list, f1_list, sf1_list = [], [], [], []\n",
    "\n",
    "    total = len(examples)\n",
    "    if total == 0:\n",
    "        # Nothing to evaluate\n",
    "        return [], 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        total_batches = (total + batch_size - 1) // batch_size\n",
    "        batch = examples[i:i + batch_size]\n",
    "        print(f\"üü¢ Processing batch {batch_num}/{total_batches} ({len(batch)} examples)\")\n",
    "        \n",
    "        for ex in batch:\n",
    "            pred = ex['prediction']\n",
    "            ref = ex['reference']\n",
    "\n",
    "            # semantic f1 (float)\n",
    "            try:\n",
    "                gold_ex = dspy.Example(question=ex['question'], response=ref)\n",
    "                pred_ex = dspy.Example(question=ex['question'], response=pred)\n",
    "                semantic_f1 = metric(gold_ex, pred_ex)  # float\n",
    "            except Exception:\n",
    "                semantic_f1 = 0.0\n",
    "\n",
    "            # token PRF\n",
    "            p, r, f = _token_prf(pred, ref)\n",
    "\n",
    "            detailed_results.append({\n",
    "                **ex,\n",
    "                'scores': {\n",
    "                    'precision': p,\n",
    "                    'recall': r,\n",
    "                    'f1': f,\n",
    "                    'semantic_f1': semantic_f1\n",
    "                }\n",
    "            })\n",
    "            pr_list.append(p); re_list.append(r); f1_list.append(f); sf1_list.append(semantic_f1)\n",
    "\n",
    "    avg_p = float(np.mean(pr_list))\n",
    "    avg_r = float(np.mean(re_list))\n",
    "    avg_f = float(np.mean(f1_list))\n",
    "    avg_sf1 = float(np.mean(sf1_list))\n",
    "\n",
    "    return detailed_results, avg_p, avg_r, avg_f, avg_sf1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
