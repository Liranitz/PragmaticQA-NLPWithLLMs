{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PragmatiCQA: Cooperative Question Answering with DSPy\n",
    "This notebook implements all parts of the PragmatiCQA assignment:\n",
    "- Data loading and analysis\n",
    "- Traditional QA baseline\n",
    "- LLM-based pragmatic QA with DSPy\n",
    "- Evaluation and comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ['XAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "# Configure DSPy with an LM FIRST (before creating SemanticF1)\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the QA model\n",
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set up SemanticF1 metric (now it will work because LM is configured)\n",
    "metric = SemanticF1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and extract questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pragmaticqa_test(dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load the val set from PragmatiCQA dataset.\"\"\"\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, \"val.jsonl\"), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "def get_first_questions(data):\n",
    "    \"\"\"Extract only the first questions from each conversation - keep topic too.\"\"\"\n",
    "    first_questions = []\n",
    "    for doc in data:\n",
    "        qas = doc.get('qas', [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        first_qa = qas[0]\n",
    "        first_questions.append({\n",
    "            'topic': doc.get('topic', ''),  # needed to choose the right retriever\n",
    "            'question': first_qa.get('q', ''),\n",
    "            'answer': first_qa.get('a', ''),\n",
    "            # support both possible field names\n",
    "            'literal_spans': [obj.get('text','') for obj in first_qa.get('a_meta', {}).get('literal_obj', [])] or first_qa.get('literal_spans', []),\n",
    "            'pragmatic_spans': [obj.get('text','') for obj in first_qa.get('a_meta', {}).get('pragmatic_obj', [])] or first_qa.get('pragmatic_spans', [])\n",
    "        })\n",
    "    return first_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 first questions from the val set.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = load_pragmaticqa_test()\n",
    "first_questions = get_first_questions(test_data)\n",
    "print(f\"Loaded {len(first_questions)} first questions from the val set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation 1 | Topic: Dinosaur ===\n",
      "\n",
      "Q1: which is the most dangerous dinosaur\n",
      "A (gold): Velociraptors were considered one of the most dangerous. \n",
      "\n",
      "Literal spans:\n",
      "['Yes']\n",
      "\n",
      "Pragmatic spans:\n",
      "['It included a Velociraptor attacking a Protoceratops ,[13] proving that '\n",
      " 'dinosaurs did indeed attack and eat each other']\n",
      "\n",
      "Q2: what about the trex?\n",
      "A (gold): Yes, it might have been the biggest terrestrial carnivore of all time. \n",
      "\n",
      "Literal spans:\n",
      "['Tyrannosaurus rex was one of, if not the terrestrial carnivores of all time']\n",
      "\n",
      "Pragmatic spans:\n",
      "['Tyrannosaurus rex was one of, if not the terrestrial carnivores of all time']\n",
      "\n",
      "=== Conversation 2 | Topic: Supernanny ===\n",
      "\n",
      "Q1: Tell what year it was released? \n",
      "A (gold): Supernanny started off as a British TV series. The first American Supernanny show began airing on ABC on January 7, 2005.\n",
      "\n",
      "Literal spans:\n",
      "['The first American Supernanny show began airing on ABC on January 7, 2005']\n",
      "\n",
      "Pragmatic spans:\n",
      "['Supernanny started off as a British TV series']\n",
      "\n",
      "Q2: When did the last episode air? \n",
      "A (gold): The last episode aired just before the COVID-19 pandemic. The show itself spanned 126 episodes contained into seven seasons.\n",
      "\n",
      "Literal spans:\n",
      "['The last episode was aired just before the COVID-19 Pandemic']\n",
      "\n",
      "Pragmatic spans:\n",
      "['The show spanned 126 episodes contained into seven seasons.']\n",
      "\n",
      "=== Conversation 3 | Topic: The Karate Kid ===\n",
      "\n",
      "Q1: Whose the main character of the Karate KId?\n",
      "A (gold): I am sorry, Dog issue. I am now fully focused.\n",
      "\n",
      "Literal spans:\n",
      "['Daniel LaRusso']\n",
      "\n",
      "Pragmatic spans:\n",
      "['17-year-old boy who moves from Newark, New Jersey to Los Angeles ']\n",
      "\n",
      "Q2: Whose the main character of the Karate KId?\n",
      "A (gold): Hide Selected Literal Answer Spans Daniel LaRusso a 17-year-old boy who moves from Newark, New Jersey to Los Angeles.Daniel starts his senior year of high school. Daniel is unhappy at having to move, make new friends and leave his old friends behind.\n",
      "\n",
      "Literal spans:\n",
      "['Daniel LaRusso',\n",
      " 'a 17-year-old boy who moves from Newark, New Jersey to Los Angeles ']\n",
      "\n",
      "Pragmatic spans:\n",
      "['Daniel starts his senior year of high school. Daniel is unhappy at having to '\n",
      " 'move, make new friends and leave his old friends behind']\n",
      "\n",
      "=== Conversation 4 | Topic: Popeye ===\n",
      "\n",
      "Q1: Is Popeye a cartoon or a character?\n",
      "A (gold): Popeye is both these, He's a character, a sailor, veteran, and was created in 1928, he's going to be a hundred this decade.\n",
      "\n",
      "Literal spans:\n",
      "['Popeye is the main protagonist of the Popeye Franchise , a sailor character '\n",
      " 'created in 1928 by Elzie Crisler Segar for his Thimble Theatre comic strip']\n",
      "\n",
      "Pragmatic spans:\n",
      "['Popeye is the main protagonist of the Popeye Franchise , a sailor character '\n",
      " 'created in 1928 by Elzie Crisler Segar for his Thimble Theatre comic strip']\n",
      "\n",
      "Q2: 100?\n",
      "A (gold): yes, in 2028, he'll reach 100. Though he is animated, most of us still think of him as being somewhat human and having an age.\n",
      "\n",
      "Literal spans:\n",
      "['Popeye is the main protagonist of the Popeye Franchise , a sailor character '\n",
      " 'created in 1928 by Elzie Crisler Segar for his Thimble Theatre comic strip']\n",
      "\n",
      "Pragmatic spans:\n",
      "['Popeye is the main protagonist of the Popeye Franchise , a sailor character '\n",
      " 'created in 1928 by Elzie Crisler Segar for his Thimble Theatre comic strip']\n",
      "\n",
      "=== Conversation 5 | Topic: Enter the Gungeon ===\n",
      "\n",
      "Q1: What's Enter the Gungeon about?\n",
      "A (gold): It's a videogame about a group of kids trying to find a gun that will kill the past. \n",
      "\n",
      "Literal spans:\n",
      "['a band of misfits seeking to shoot, loot, dodge roll and table-flip their '\n",
      " 'way to personal absolution by reaching the legendary Gungeon’s ultimate '\n",
      " 'treasure: the gun that can kill the past']\n",
      "\n",
      "Pragmatic spans:\n",
      "[\"The game's spin-off Exit the Gungeon was released for the Apple Arcade in \"\n",
      " '2019, and released for PC and Switch in 2020.']\n",
      "\n",
      "Q2: Which system can you play it on?\n",
      "A (gold): It's available for PC, M\n",
      "\n",
      "Literal spans:\n",
      "['PC, Mac, Linux, PlayStation 4, Xbox One, Nintendo Switch']\n",
      "\n",
      "Pragmatic spans:\n",
      "[\"The game's spin-off Exit the Gungeon was released for the Apple Arcade in \"\n",
      " '2019, and released for PC and Switch in 2020.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# choose 5 random conversations from the test set\n",
    "examples = random.sample(test_data, 5)\n",
    "\n",
    "for i, conv in enumerate(examples, 1):\n",
    "    print(f\"\\n=== Conversation {i} | Topic: {conv['topic']} ===\")\n",
    "    for j, qa in enumerate(conv['qas'][:2]):  # show first 2 Q/A pairs per topic\n",
    "        print(f\"\\nQ{j+1}: {qa['q']}\")\n",
    "        print(f\"A (gold): {qa['a']}\")\n",
    "        lit = [o['text'] for o in qa['a_meta']['literal_obj']]\n",
    "        prag = [o['text'] for o in qa['a_meta']['pragmatic_obj']]\n",
    "        print(\"\\nLiteral spans:\")\n",
    "        pprint(lit[:2])  # show up to 2\n",
    "        print(\"\\nPragmatic spans:\")\n",
    "        pprint(prag[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 . Motivation and Contributions of the PragmatiCQA Paper\n",
    "The PragmatiCQA dataset was created to evaluate how well language models go beyond literal QA and demonstrate *pragmatic reasoning* - the ability to infer a speaker’s hidden intent, anticipate follow-up questions, and provide cooperative, conversational answers.  \n",
    "Unlike classical QA benchmarks that simply require extracting the correct fact span, PragmatiCQA tests whether a system can behave like a helpful conversational partner: it must reason about **what the user probably wants to know next** and include that information proactively.  \n",
    "The dataset thus connects pragmatic inference with *Theory of Mind*—understanding the beliefs and information gaps of another agent.\n",
    "\n",
    "### 2 . Why This Dataset Is Challenging\n",
    "1. **Implicit Goals:** The literal answer is rarely sufficient; useful responses require inferring unstated goals.  \n",
    "2. **Asymmetric Information:** The model (teacher) has access to corpus documents the user does not, so it must decide what extra content is relevant to share.  \n",
    "3. **Conversational Dependence:** Later turns depend on prior ones—context tracking and reasoning over multiple utterances are essential.  \n",
    "4. **Rich Evaluation:** Answers are open-ended, so automatic metrics (Semantic F1) must judge informational overlap rather than exact string matches.\n",
    "\n",
    "### 3 . Qualitative Examples from the Dataset\n",
    "\n",
    "#### Example 1 – *Dinosaur*\n",
    "| Aspect | Literal | Pragmatic Enrichment |\n",
    "|--------|----------|--------------------|\n",
    "| Q1: “Which is the most dangerous dinosaur?” | “Yes.” *(unhelpful minimal answer)* | Adds a vivid detail: “Velociraptor attacking a Protoceratops, proving that dinosaurs attacked and ate each other.” → Explains *why* it’s considered dangerous. |\n",
    "| Q2: “What about the T-Rex?” | Identifies T-Rex as one of the largest carnivores. | Expands to pragmatic inference – emphasizes that it *might have been the biggest terrestrial carnivore of all time*, satisfying curiosity about comparative danger. |\n",
    "\n",
    "#### Example 2 – *Supernanny*\n",
    "| Aspect | Literal | Pragmatic Enrichment |\n",
    "|--------|----------|--------------------|\n",
    "| Q1 (Year released) | States U.S. air date. | Adds British-origin context → anticipates follow-up “where did it start?” |\n",
    "| Q2 (Last episode) | Gives date only. | Adds meta info (“126 episodes over 7 seasons”) → shows scope and longevity of the show, not asked explicitly but conversationally relevant. |\n",
    "\n",
    "#### Example 3 – *The Karate Kid*\n",
    "| Aspect | Literal | Pragmatic Enrichment |\n",
    "|--------|----------|--------------------|\n",
    "| Q1 (Main character) | “Daniel LaRusso.” | Adds age + relocation story → explains who he is and why he matters. |\n",
    "| Q2 (Repeated question) | Name + move details. | Adds emotional state → contextualizes the character’s journey, showing empathy and narrative understanding. |\n",
    "\n",
    "#### Example 4 – *Popeye*\n",
    "| Aspect | Literal | Pragmatic Enrichment |\n",
    "|--------|----------|--------------------|\n",
    "| Q1 (Cartoon or character?) | “A sailor character created in 1928.” | Interprets mixed intent → affirms *both* (“He’s a character and a cartoon”) and anticipates curiosity about his age (“going to be 100 this decade”). |\n",
    "| Q2 (“100?”) | Repeats factual creation date. | Clarifies temporal reasoning → adds human-like commentary (“Though he’s animat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional NLP Baseline for PragmatiCQA\n",
    "\n",
    "Implementing a baseline QA system using a pre-trained model from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional NLP Baseline\n",
    "\n",
    "In this section, we implement a baseline question-answering system using a pre-trained transformer model. The baseline will use a DistilBERT model fine-tuned on SQuAD to extract answers from context without any special multi-step reasoning. We will evaluate this model on the PragmatiCQA validation set by providing different contexts and measuring performance with an LLM-based Semantic F1 metric. Specifically, we compare three scenarios for each question: using the gold literal spans as context, using the gold pragmatic spans as context, and using a retrieved context from the wiki corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Model Initialization\n",
    "\n",
    "We begin by loading necessary libraries and configuring the environment. This includes setting up the DSPy framework with an API key (for LLM access in evaluation) and initializing the HuggingFace QA pipeline with the DistilBERT model (distilbert-base-cased-distilled-squad). We also instantiate the SemanticF1 metric for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded aliases -> existing folder names\n",
    "TOPIC_ALIASES = {\n",
    "    \"A Nightmare on Elm Street (2010 film)\": \"A Nightmare on Elm Street\",\n",
    "    \"Alexander Hamilton\": \"Hamilton the Musical\",      # proxy topic\n",
    "    \"Popeye\": \"Popeye the Sailor\",\n",
    "    \"The Wonderful Wizard of Oz (book)\": \"Wizard of Oz\",\n",
    "}\n",
    "\n",
    "import os\n",
    "\n",
    "def list_available_topics(sources_dir):\n",
    "    return {\n",
    "        name for name in os.listdir(sources_dir)\n",
    "        if os.path.isdir(os.path.join(sources_dir, name))\n",
    "    }\n",
    "\n",
    "def resolve_topic_name(topic, available_topics):\n",
    "    \"\"\"Return a folder topic to use for this logical topic.\"\"\"\n",
    "    # exact match first\n",
    "    if topic in available_topics:\n",
    "        return topic\n",
    "    # alias match next\n",
    "    alias = TOPIC_ALIASES.get(topic)\n",
    "    if alias and alias in available_topics:\n",
    "        return alias\n",
    "    # nothing found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# P2.2 - Load HTML sources and build per-topic retrievers\n",
    "\n",
    "def create_retriever_for_topic(topic, sources_dir=\"../PragmatiCQA-sources\", k=5):\n",
    "    topic_dir = os.path.join(sources_dir, topic) if topic else None\n",
    "    corpus = read_html_files_with_stopper(topic_dir) if topic_dir else []\n",
    "    if not corpus:\n",
    "        return None\n",
    "    return dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=k)\n",
    "\n",
    "def cache_topic_retrievers(split_data, sources_dir=\"../PragmatiCQA-sources\", k=5):\n",
    "    topics = {ex.get(\"topic\",\"\") for ex in split_data if ex.get(\"topic\",\"\")}\n",
    "    available = list_available_topics(sources_dir)\n",
    "    cache = {}\n",
    "    missing = []\n",
    "\n",
    "    for t in sorted(topics):\n",
    "        folder_topic = resolve_topic_name(t, available) or t\n",
    "        r = create_retriever_for_topic(folder_topic, sources_dir=sources_dir, k=k)\n",
    "        if r is not None:\n",
    "            # keep key as the ORIGINAL topic from the dataset,\n",
    "            # but build the retriever from the resolved folder topic\n",
    "            cache[t] = r\n",
    "        else:\n",
    "            missing.append(t)\n",
    "\n",
    "    print(f\"Built retrievers for {len(cache)}/{len(topics)} topics.\")\n",
    "    if missing:\n",
    "        print(\"Missing or empty topic folders:\", missing)\n",
    "    return cache\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "def read_html_files_with_stopper(directory):\n",
    "    docs = []\n",
    "    if not os.path.isdir(directory):\n",
    "        return docs\n",
    "    for fn in os.listdir(directory):\n",
    "        if fn.endswith(\".html\"):\n",
    "            p = os.path.join(directory, fn)\n",
    "            try:\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "                    text = soup.get_text(separator=\" \", strip=True)\n",
    "                    if text:\n",
    "                        # clamp to avoid huge prompts\n",
    "                        docs.append(text[:3000])\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {fn}: {e}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built retrievers for 11/11 topics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build per-topic retrievers from the topics present in this split\n",
    "retr_lookup_val = cache_topic_retrievers(test_data, sources_dir=\"../PragmatiCQA-sources\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics missing retrievers (4): ['A Nightmare on Elm Street (2010 film)', 'Alexander Hamilton', 'Popeye', 'The Wonderful Wizard of Oz (book)']\n"
     ]
    }
   ],
   "source": [
    "topics_in_data = {ex.get(\"topic\",\"\") for ex in test_data if ex.get(\"topic\",\"\")}\n",
    "topics_in_cache = set(retr_lookup_val.keys())\n",
    "missing = topics_in_data - topics_in_cache\n",
    "print(f\"Topics missing retrievers ({len(missing)}): {sorted(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 73 topic folders under:\n",
      "C:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\PragmatiCQA-sources\n",
      "\n",
      " - 'Cats' Musical\n",
      " - A Nightmare on Elm Street\n",
      " - Arrowverse\n",
      " - Barney\n",
      " - Baseball\n",
      " - Batman\n",
      " - Big Nate\n",
      " - Bleach\n",
      " - Britney Spears\n",
      " - Detective Conan\n",
      " - Dinosaur\n",
      " - Doctor Who\n",
      " - Doom Patrol\n",
      " - Dr. Stone\n",
      " - Dream Team\n",
      " - Edens Zero\n",
      " - Enter the Gungeon\n",
      " - Evangelion\n",
      " - Fallout\n",
      " - Fullmetal Alchemist\n",
      " - Game of Thrones\n",
      " - Girl Genius\n",
      " - Goosebumps\n",
      " - H. P. Lovecraft\n",
      " - Half-Life series\n",
      " - Halo\n",
      " - Hamilton the Musical\n",
      " - Harry Potter\n",
      " - Inazuma Eleven\n",
      " - Indiana Jones\n",
      " - Jujutsu Kaisen\n",
      " - Kingdom Hearts\n",
      " - Kung Fu Panda\n",
      " - LEGO\n",
      " - Lady Gaga\n",
      " - Lemony Snicket\n",
      " - MS Paint Adventures\n",
      " - Madagascar\n",
      " - My Hero Academia\n",
      " - Mystery Science Theater 3000\n",
      " - Non-alien Creatures\n",
      " - Olympics\n",
      " - One Piece\n",
      " - Peanuts Comics\n",
      " - Pixar\n",
      " - Popeye the Sailor\n",
      " - Rap\n",
      " - Reborn\n",
      " - Riordan\n",
      " - Serious Sam\n",
      " - Shaman King\n",
      " - ShowBiz Pizza\n",
      " - Six the Musical\n",
      " - Skulduggery Pleasant\n",
      " - Sonic the Hedgehog\n",
      " - Soul Eater\n",
      " - Stargate\n",
      " - Studio Ghibli\n",
      " - Supernanny\n",
      " - Taylor Swift\n",
      " - The BIONICLE\n",
      " - The Dark Crystal\n",
      " - The Formula 1\n",
      " - The Grisha universe\n",
      " - The Karate Kid\n",
      " - The Legend of Zelda\n",
      " - The Matrix\n",
      " - The Maze Runner\n",
      " - The Wheel of Time\n",
      " - Throne of Glass\n",
      " - Umbrella Academy\n",
      " - Wings of Fire\n",
      " - Wizard of Oz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sources_root = r\"C:\\liran\\Program\\SMSTR8\\NLPWithLLM\\hw3\\PragmatiCQA-sources\"\n",
    "\n",
    "if not os.path.exists(sources_root):\n",
    "    print(f\"❌ Path not found: {sources_root}\")\n",
    "else:\n",
    "    folders = [name for name in os.listdir(sources_root)\n",
    "               if os.path.isdir(os.path.join(sources_root, name))]\n",
    "    print(f\"📁 Found {len(folders)} topic folders under:\\n{sources_root}\\n\")\n",
    "    for f in sorted(folders):\n",
    "        print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETR_TOPK = 3\n",
    "CTX_MAX_CHARS = 2000\n",
    "\n",
    "def to_strings(passages):\n",
    "    out = []\n",
    "    for p in passages:\n",
    "        if isinstance(p, str):\n",
    "            out.append(p)\n",
    "        elif hasattr(p, \"text\"):\n",
    "            out.append(str(p.text))\n",
    "        else:\n",
    "            out.append(str(p))\n",
    "    return out\n",
    "\n",
    "def get_retrieved_context(question, retr):\n",
    "    \"\"\"Return joined context string from a DSPy retriever result, robust to shapes.\"\"\"\n",
    "    try:\n",
    "        res = retr(question)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) list[str] already\n",
    "    if isinstance(res, list):\n",
    "        passages = res\n",
    "\n",
    "    # 2) has attribute .passages\n",
    "    elif hasattr(res, \"passages\"):\n",
    "        passages = res.passages\n",
    "\n",
    "    # 3) dict with 'passages'\n",
    "    elif isinstance(res, dict) and \"passages\" in res:\n",
    "        passages = res[\"passages\"]\n",
    "\n",
    "    # 4) fallback - stringify\n",
    "    else:\n",
    "        passages = [str(res)]\n",
    "\n",
    "    strings = to_strings(passages)[:RETR_TOPK]\n",
    "    ctx = \" \".join(s.strip() for s in strings if s and s.strip())\n",
    "    return ctx[:CTX_MAX_CHARS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "def _token_prf(prediction, reference):\n",
    "    pt = set(str(prediction).lower().split())\n",
    "    rt = set(str(reference).lower().split())\n",
    "    if not pt or not rt:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    overlap = pt & rt\n",
    "    precision = len(overlap) / len(pt) if pt else 0.0\n",
    "    recall    = len(overlap) / len(rt) if rt else 0.0\n",
    "    f1 = 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_qa_system_full_metrics(\n",
    "    questions,\n",
    "    context_type='retrieved',\n",
    "    batch_size=16,\n",
    "    retr_lookup=None,\n",
    "    qa_pipeline=None,\n",
    "    metric: SemanticF1=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates with:\n",
    "    - DSPy SemanticF1 using dspy.Example (LLM-as-judge)\n",
    "    - token precision, recall, F1\n",
    "    Returns: (detailed_results, avg_precision, avg_recall, avg_f1, avg_semantic_f1)\n",
    "    \"\"\"\n",
    "    if context_type == 'retrieved' and not isinstance(retr_lookup, dict):\n",
    "        raise ValueError(\"context_type='retrieved' requires retr_lookup=dict.\")\n",
    "    if qa_pipeline is None:\n",
    "        raise ValueError(\"qa_pipeline must be provided.\")\n",
    "    if metric is None:\n",
    "        # make sure you did dspy.configure(lm=...) before this\n",
    "        metric = SemanticF1()\n",
    "\n",
    "    examples = []\n",
    "    empty_ctx = 0\n",
    "\n",
    "    # build contexts and predictions\n",
    "    for q in questions:\n",
    "        question  = q['question']\n",
    "        reference = q['answer']\n",
    "\n",
    "        if context_type == 'literal':\n",
    "            context = ' '.join(q.get('literal_spans', [])).strip()\n",
    "        elif context_type == 'pragmatic':\n",
    "            context = ' '.join(q.get('pragmatic_spans', [])).strip()\n",
    "        elif context_type == 'retrieved':\n",
    "            topic = q.get('topic', '')\n",
    "            retr = retr_lookup.get(topic) if retr_lookup else None\n",
    "            context = get_retrieved_context(q['question'], retr) if retr else ''\n",
    "\n",
    "        if not context:\n",
    "            empty_ctx += 1\n",
    "            pred = \"\"\n",
    "        else:\n",
    "            try:\n",
    "                out = qa_pipeline(question=question, context=context)\n",
    "                pred = (out.get('answer', '') if isinstance(out, dict) else \"\").strip()\n",
    "            except Exception:\n",
    "                pred = \"\"\n",
    "\n",
    "        examples.append({\n",
    "            'topic': q.get('topic',''),\n",
    "            'question': question,\n",
    "            'prediction': pred,\n",
    "            'reference': reference,\n",
    "            'context_type': context_type,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "    if verbose:\n",
    "        n = len(examples)\n",
    "        print(f\"Built {n} examples - empty contexts: {empty_ctx}/{n} ({empty_ctx/max(1,n):.1%})\")\n",
    "\n",
    "    if not examples:\n",
    "        return [], 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # per-example SemanticF1 using dspy.Example\n",
    "    detailed_results = []\n",
    "    pr_list, re_list, f1_list, sf1_list = [], [], [], []\n",
    "\n",
    "    total = len(examples)\n",
    "    for i in range(0, total, batch_size):\n",
    "        if verbose:\n",
    "            print(f\"scoring batch {i//batch_size+1}/{(total+batch_size-1)//batch_size}\")\n",
    "        batch = examples[i:i+batch_size]\n",
    "        for ex in batch:\n",
    "            pred = ex['prediction']\n",
    "            ref  = ex['reference']\n",
    "\n",
    "            # semantic-F1 via dspy.Example\n",
    "            try:\n",
    "                gold_ex = dspy.Example(question=ex['question'], response=ref)\n",
    "                pred_ex = dspy.Example(question=ex['question'], response=pred)\n",
    "                s_f1 = float(metric(gold_ex, pred_ex))\n",
    "            except Exception:\n",
    "                s_f1 = 0.0\n",
    "\n",
    "            # token PRF\n",
    "            p, r, f = _token_prf(pred, ref)\n",
    "\n",
    "            detailed_results.append({\n",
    "                **ex,\n",
    "                'scores': {\n",
    "                    'precision': p,\n",
    "                    'recall': r,\n",
    "                    'f1': f,\n",
    "                    'semantic_f1': s_f1\n",
    "                }\n",
    "            })\n",
    "            pr_list.append(p); re_list.append(r); f1_list.append(f); sf1_list.append(s_f1)\n",
    "\n",
    "    avg_p   = float(np.mean(pr_list)) if pr_list else 0.0\n",
    "    avg_r   = float(np.mean(re_list)) if re_list else 0.0\n",
    "    avg_f   = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    avg_sf1 = float(np.mean(sf1_list)) if sf1_list else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Mean PRF: P={avg_p:.3f} R={avg_r:.3f} F1={avg_f:.3f} | SemanticF1={avg_sf1:.3f}\")\n",
    "\n",
    "    return detailed_results, avg_p, avg_r, avg_f, avg_sf1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Compute precision, recall, F1, and SemanticF1 across all first-turn questions in the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating literal...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.522 R=0.074 F1=0.118 | SemanticF1=0.424\n",
      "\n",
      "Evaluating pragmatic...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.563 R=0.091 F1=0.143 | SemanticF1=0.383\n",
      "\n",
      "Evaluating retrieved...\n",
      "Built 179 examples - empty contexts: 0/179 (0.0%)\n",
      "scoring batch 1/6\n",
      "scoring batch 2/6\n",
      "scoring batch 3/6\n",
      "scoring batch 4/6\n",
      "scoring batch 5/6\n",
      "scoring batch 6/6\n",
      "Mean PRF: P=0.230 R=0.039 F1=0.062 | SemanticF1=0.149\n"
     ]
    }
   ],
   "source": [
    "metric = SemanticF1()  # created after configure\n",
    "\n",
    "clean_results = {}\n",
    "for cfg in ['literal','pragmatic','retrieved']:\n",
    "    print(f\"\\nEvaluating {cfg}...\")\n",
    "    kwargs = {'retr_lookup': retr_lookup_val} if cfg == 'retrieved' else {}\n",
    "    detailed, avg_p, avg_r, avg_f, avg_sf1 = evaluate_qa_system_full_metrics(\n",
    "        first_questions,\n",
    "        context_type=cfg,\n",
    "        batch_size=32,\n",
    "        retr_lookup=kwargs.get('retr_lookup'),\n",
    "        qa_pipeline=qa_pipeline,\n",
    "        metric=metric,\n",
    "        verbose=True\n",
    "    )\n",
    "    clean_results[cfg] = {\n",
    "        'avg_precision': avg_p,\n",
    "        'avg_recall': avg_r,\n",
    "        'avg_f1': avg_f,\n",
    "        'avg_semantic_f1': avg_sf1,\n",
    "        'detailed_results': detailed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to clean_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "with open(\"clean_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_results, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved to clean_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Overall Evaluation Report\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>semantic_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Literal</th>\n",
       "      <td>0.522104</td>\n",
       "      <td>0.073627</td>\n",
       "      <td>0.118310</td>\n",
       "      <td>0.424383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pragmatic</th>\n",
       "      <td>0.563388</td>\n",
       "      <td>0.090601</td>\n",
       "      <td>0.143291</td>\n",
       "      <td>0.383401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrieved</th>\n",
       "      <td>0.230119</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.149340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           precision    recall        f1  semantic_f1\n",
       "config                                               \n",
       "Literal     0.522104  0.073627  0.118310     0.424383\n",
       "Pragmatic   0.563388  0.090601  0.143291     0.383401\n",
       "Retrieved   0.230119  0.038806  0.062257     0.149340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_num(x, default=0.0):\n",
    "    try:\n",
    "        return float(x) if not (x is None or (isinstance(x, float) and math.isnan(x))) else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def report_overall_metrics(results):\n",
    "    \"\"\"Prints and returns a small dataframe with avg precision/recall/F1/semantic-F1 per configuration.\"\"\"\n",
    "    rows = []\n",
    "    for config, res in results.items():\n",
    "        rows.append({\n",
    "            \"config\": config.capitalize(),\n",
    "            \"precision\": _safe_num(res.get(\"avg_precision\")),\n",
    "            \"recall\": _safe_num(res.get(\"avg_recall\")),\n",
    "            \"f1\": _safe_num(res.get(\"avg_f1\")),\n",
    "            \"semantic_f1\": _safe_num(res.get(\"avg_semantic_f1\")),\n",
    "        })\n",
    "    df = pd.DataFrame(rows).set_index(\"config\").sort_index()\n",
    "    print(\"\\n📊 Overall Evaluation Report\\n\" + \"=\"*40)\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "overall_df = report_overall_metrics(clean_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, metric_key='semantic_f1', topk=1):\n",
    "    \"\"\"\n",
    "    Prints best and worst examples per config by a chosen metric.\n",
    "    metric_key in {'semantic_f1','f1','precision','recall'}.\n",
    "    \"\"\"\n",
    "    for config, bundle in results.items():\n",
    "        detailed = bundle.get('detailed_results', [])\n",
    "        if not detailed:\n",
    "            print(f\"\\n⚠️ No detailed results for configuration '{config}'\")\n",
    "            continue\n",
    "\n",
    "        # collect (score, idx)\n",
    "        scored = []\n",
    "        for i, ex in enumerate(detailed):\n",
    "            val = ex.get('scores', {}).get(metric_key, 0.0)\n",
    "            try:\n",
    "                val = float(val)\n",
    "            except Exception:\n",
    "                val = 0.0\n",
    "            if math.isnan(val):\n",
    "                val = 0.0\n",
    "            scored.append((val, i))\n",
    "\n",
    "        if not scored:\n",
    "            print(f\"\\n⚠️ No '{metric_key}' scores found for '{config}'\")\n",
    "            continue\n",
    "\n",
    "        scored.sort(key=lambda t: t[0], reverse=True)\n",
    "        best = scored[:topk]\n",
    "        worst = list(reversed(scored))[:topk]\n",
    "\n",
    "        print(f\"\\n{'='*60}\\n🔹 Analysis for '{config}' configuration ({metric_key.upper()})\\n{'='*60}\")\n",
    "\n",
    "        print(\"\\n✅ Best example(s):\")\n",
    "        for val, idx in best:\n",
    "            ex = detailed[idx]\n",
    "            print(f\"- Score: {val:.4f}\")\n",
    "            print(f\"  Q: {ex['question']}\")\n",
    "            print(f\"  Pred: {ex['prediction']}\")\n",
    "            print(f\"  Ref: {ex['reference']}\")\n",
    "            print(f\"  Ctx: {ex['context'][:200]}...\\n\")\n",
    "\n",
    "        print(\"❌ Worst example(s):\")\n",
    "        for val, idx in worst:\n",
    "            ex = detailed[idx]\n",
    "            print(f\"- Score: {val:.4f}\")\n",
    "            print(f\"  Q: {ex['question']}\")\n",
    "            print(f\"  Pred: {ex['prediction']}\")\n",
    "            print(f\"  Ref: {ex['reference']}\")\n",
    "            print(f\"  Ctx: {ex['context'][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, metric_key='semantic_f1'):\n",
    "    \"\"\"\n",
    "    Analyze where the model succeeds and fails across configurations.\n",
    "    Args:\n",
    "        results (dict): output of evaluate_qa_system_full_metrics() runs.\n",
    "        metric_key (str): which metric to analyze (e.g., 'semantic_f1', 'f1', 'precision', 'recall')\n",
    "    \"\"\"\n",
    "    for config in results:\n",
    "        detailed = results[config].get('detailed_results', [])\n",
    "        if not detailed:\n",
    "            print(f\"\\n⚠️ No detailed results for configuration '{config}'\")\n",
    "            continue\n",
    "\n",
    "        # Extract chosen metric for each example\n",
    "        scores = [ex['scores'].get(metric_key, 0.0) for ex in detailed]\n",
    "\n",
    "        if not scores:\n",
    "            print(f\"\\n⚠️ No '{metric_key}' scores found for '{config}'\")\n",
    "            continue\n",
    "\n",
    "        best_idx = int(max(range(len(scores)), key=lambda i: scores[i]))\n",
    "        worst_idx = int(min(range(len(scores)), key=lambda i: scores[i]))\n",
    "\n",
    "        best_example = detailed[best_idx]\n",
    "        worst_example = detailed[worst_idx]\n",
    "\n",
    "        print(f\"\\n{'='*60}\\n🔹 Analysis for '{config}' configuration ({metric_key.upper()})\\n{'='*60}\")\n",
    "\n",
    "        print(\"\\n✅ Best performing example:\")\n",
    "        print(f\"Question: {best_example['question']}\")\n",
    "        print(f\"Prediction: {best_example['prediction']}\")\n",
    "        print(f\"Reference: {best_example['reference']}\")\n",
    "        print(f\"Context snippet: {best_example['context'][:150]}...\")\n",
    "        print(f\"{metric_key}: {best_example['scores'][metric_key]:.4f}\")\n",
    "\n",
    "        print(\"\\n❌ Worst performing example:\")\n",
    "        print(f\"Question: {worst_example['question']}\")\n",
    "        print(f\"Prediction: {worst_example['prediction']}\")\n",
    "        print(f\"Reference: {worst_example['reference']}\")\n",
    "        print(f\"Context snippet: {worst_example['context'][:150]}...\")\n",
    "        print(f\"{metric_key}: {worst_example['scores'][metric_key]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results (success/failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔹 Analysis for 'literal' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "✅ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: what year was the show release ? \n",
      "  Pred: 2005\n",
      "  Ref: Good morning!The first American Supernanny show began airing on ABC on January 7, 2005.\n",
      "  Ctx: The first American Supernanny show began airing on ABC on January 7, 2005....\n",
      "\n",
      "❌ Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: How many books have been published in the Game of Thrones series so far?\n",
      "  Pred: Yes\n",
      "  Ref: There are 5 books in the series and 3 prequel novellas set in the same world.\n",
      "  Ctx: Yes...\n",
      "\n",
      "\n",
      "============================================================\n",
      "🔹 Analysis for 'pragmatic' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "✅ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: Ok, Where does the Supernanny mainly live (country)?\n",
      "  Pred: British\n",
      "  Ref: Supernanny lives in the UK. It is originally a British TV series.\n",
      "  Ctx: a British TV series...\n",
      "\n",
      "❌ Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: What is Game of thrones its real or not?\n",
      "  Pred: George R.R. Martin\n",
      "  Ref: It is based on the novel series A Song of Ice and Fire\n",
      "  Ctx: written by George R.R. Martin...\n",
      "\n",
      "\n",
      "============================================================\n",
      "🔹 Analysis for 'retrieved' configuration (SEMANTIC_F1)\n",
      "============================================================\n",
      "\n",
      "✅ Best example(s):\n",
      "- Score: 1.0000\n",
      "  Q: How many books are in the Game of Thrones series?\n",
      "  Pred: five\n",
      "  Ref: Game of Thrones, the HBO series is based on the five books of George R. R. Martin's Song of Fire and Ice series.\n",
      "  Ctx: A Song of Ice and Fire is an award-winning series of best-selling books of epic fantasy novels by American author and scriptwriter George R.R. Martin . The series currently comprises five published no...\n",
      "\n",
      "❌ Worst example(s):\n",
      "- Score: 0.0000\n",
      "  Q: What is Game of thrones its real or not?\n",
      "  Pred: The Iron Throne\n",
      "  Ref: It is based on the novel series A Song of Ice and Fire\n",
      "  Ctx: Iron Throne Season(s) 1\n",
      "      ,\n",
      "      2\n",
      "      ,\n",
      "      3\n",
      "      ,\n",
      "      4\n",
      "      ,\n",
      "      5\n",
      "      ,\n",
      "      6\n",
      "      ,\n",
      "      7\n",
      "      ,\n",
      "      8 Status Destroyed (melted by Drogon ) Type Throne\n",
      "      Ruling in...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check if clean_results exists, otherwise load from file\n",
    "if 'clean_results' not in globals() or not clean_results:\n",
    "    if os.path.exists(\"clean_results.json\"):\n",
    "        with open(\"clean_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            clean_results = json.load(f)\n",
    "        print(\"Loaded clean_results from clean_results.json\")\n",
    "    else:\n",
    "        raise RuntimeError(\"clean_results is not initialized and clean_results.json not found.\")\n",
    "\n",
    "analyze_results(clean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Summary:**  \n",
    "From the above results, the baseline model exhibits very low recall in all cases, meaning it often only captures a small part of the gold answer. Using the literal spans as context yields the highest Semantic F1 (≈0.424), while using pragmatic spans gives a slightly lower Semantic F1 (≈0.383) but a small boost in precision and recall, indicating the added information helps cover more of the answer. When using the retriever’s passages, performance drops sharply (Semantic F1 ≈0.149) – this suggests that without the oracle context, the baseline struggles, likely due to irrelevant or insufficient information retrieved. Overall, the DistilBERT baseline tends to give very brief answers (high precision, very low recall) and often misses the richer details needed for a truly cooperative answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The LLM Multi-Step Prompting Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we develop a more advanced question-answering system that uses a large language model with multi-step reasoning to produce pragmatic, cooperative answers. This approach leverages the conversation history and retrieved context to infer the user’s underlying needs and provide more informative answers. We implement the multi-step pipeline using the DSPy framework, creating a custom module (called PragmaticRAG) that will:\n",
    "\n",
    "* Take into account the full conversation (all previous Q&A pairs) along with the current question.\n",
    "\n",
    "* Use the retriever to gather relevant information for the question.\n",
    "\n",
    "* Perform intermediate reasoning (e.g., summarizing user interests and identifying pragmatic needs).\n",
    "\n",
    "* Possibly generate a follow-up (cooperative) query for additional context.\n",
    "\n",
    "* Finally, generate a well-rounded answer that combines literal and pragmatic content.\n",
    "\n",
    "This multi-step approach will be evaluated on the PragmatiCQA validation set for all turns of each conversation (not just the first question). We will also compare its performance on first-turn questions to the baseline from Part 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Multi-Turn Conversation Data\n",
    "\n",
    "Before building the model, we prepare the dataset in a suitable format. We need each question along with its conversation context. The function below, get_all_questions, converts the dataset into a list of entries, where each entry contains:\n",
    "\n",
    "* the topic,\n",
    "\n",
    "* the current question,\n",
    "\n",
    "* the ground-truth answer(s),\n",
    "\n",
    "* the conversation history (all previous question-answer pairs in the dialogue).\n",
    "\n",
    "This will allow the model to consider prior turns when answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.3 - Turn builders\n",
    "def get_all_questions(split_data):\n",
    "    out = []\n",
    "    for conv in split_data:\n",
    "        topic = conv.get(\"topic\",\"\")\n",
    "        history = []\n",
    "        for qa in conv.get(\"qas\", []):\n",
    "            q = qa[\"q\"]; a = qa[\"a\"]\n",
    "            out.append({\n",
    "                \"topic\": topic,\n",
    "                \"question\": q,\n",
    "                \"answers\": [a],\n",
    "                \"conversation_history\": list(history)   # copy current history\n",
    "            })\n",
    "            history.append((q, a))  # update rolling history\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the Cooperative QA Modules with DSPy\n",
    "\n",
    "We now design a DSPy program composed of several sub-modules to handle different steps of reasoning:\n",
    "\n",
    "* ConversationAnalyzer – analyzes the dialogue history and current question to produce a summary of the user’s interests or the conversation’s goal.\n",
    "\n",
    "* PragmaticReasoner – reasons about the current question and retrieved context to identify pragmatic needs (what additional information the user might be looking for) and formulates a cooperative follow-up query to fetch that information.\n",
    "\n",
    "* CooperativeAnswerGenerator – generates the final answer by combining the literal answer context, the additionally retrieved context, and the inferred pragmatic needs, producing a comprehensive response.\n",
    "\n",
    "* PragmaticRAG – the top-level module that ties everything together. It uses the above components and the retriever to answer questions pragmatically:\n",
    "\n",
    "    1. Initial retrieval: get an initial set of passages (base_ctx) by querying the retriever with the literal question.\n",
    "\n",
    "    2. Conversation analysis: use ConversationAnalyzer to get user_interests (and possibly a high-level goal) from the conversation history and question.\n",
    "\n",
    "    3. Pragmatic reasoning: use PragmaticReasoner to identify pragmatic_needs and propose a cooperative_query based on the conversation and the initial retrieved context.\n",
    "\n",
    "    4. Cooperative retrieval: use the cooperative_query to retrieve additional passages (coop_ctx) that might address the implicit needs.\n",
    "\n",
    "    5. Answer generation: feed the question, inferred interests, pragmatic needs, and both sets of context (literal and pragmatic) into CooperativeAnswerGenerator to produce the final answer.\n",
    "\n",
    "The code below defines these modules as DSPy Module classes with Chain-of-Thought prompts (the prompt templates are specified in the dspy.ChainOfThought strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.4 - Multi-step DSPy program\n",
    "class ConversationAnalyzer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyze = dspy.ChainOfThought(\n",
    "            \"conversation_history, current_question -> user_interests, conversation_goal\"\n",
    "        )\n",
    "    def forward(self, conversation_history, current_question):\n",
    "        return self.analyze(conversation_history=conversation_history,\n",
    "                            current_question=current_question)\n",
    "\n",
    "class PragmaticReasoner(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reason = dspy.ChainOfThought(\n",
    "            \"conversation_history, current_question, retrieved_context -> pragmatic_needs, cooperative_query\"\n",
    "        )\n",
    "    def forward(self, conversation_history, current_question, retrieved_context):\n",
    "        return self.reason(conversation_history=conversation_history,\n",
    "                           current_question=current_question,\n",
    "                           retrieved_context=retrieved_context)\n",
    "\n",
    "class CooperativeAnswerGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(\n",
    "            \"question, user_interests, pragmatic_needs, literal_context, pragmatic_context -> response\"\n",
    "        )\n",
    "    def forward(self, question, user_interests, pragmatic_needs, literal_context, pragmatic_context):\n",
    "        return self.generate(question=question,\n",
    "                             user_interests=user_interests,\n",
    "                             pragmatic_needs=pragmatic_needs,\n",
    "                             literal_context=literal_context,\n",
    "                             pragmatic_context=pragmatic_context)\n",
    "\n",
    "class PragmaticRAG(dspy.Module):\n",
    "    def __init__(self, retriever_lookup, default_k=5, max_context_chars=12000):\n",
    "        super().__init__()\n",
    "        self.retriever_lookup = retriever_lookup\n",
    "        self.default_k = default_k\n",
    "        self.max_context_chars = max_context_chars\n",
    "        self.conv = ConversationAnalyzer()\n",
    "        self.reason = PragmaticReasoner()\n",
    "        self.answerer = CooperativeAnswerGenerator()\n",
    "\n",
    "    def _retrieve(self, topic, query, k=None):\n",
    "        retr = self.retriever_lookup.get(topic)\n",
    "        if retr is None or not query:\n",
    "            return []\n",
    "        retr.k = k or self.default_k\n",
    "        try:\n",
    "            res = retr(query)\n",
    "            passages = list(res.passages) if hasattr(res, \"passages\") else []\n",
    "            return passages\n",
    "        except Exception as e:\n",
    "            print(\"Retrieval error:\", e)\n",
    "            return []\n",
    "\n",
    "    def _join_ctx(self, chunks):\n",
    "        if not chunks:\n",
    "            return \"\"\n",
    "        text = \" \".join(chunks)\n",
    "        # clamp to keep prompts reasonable\n",
    "        return text[: self.max_context_chars]\n",
    "\n",
    "    def forward(self, topic, conversation_history, question, k=None):\n",
    "        # 1) base retrieval on the literal question\n",
    "        base_ctx_chunks = self._retrieve(topic, question, k=k)\n",
    "        base_ctx = self._join_ctx(base_ctx_chunks)\n",
    "\n",
    "        # 2) analyze conversation\n",
    "        conv_out = self.conv(conversation_history=conversation_history,\n",
    "                             current_question=question)\n",
    "        user_interests = getattr(conv_out, \"user_interests\", \"\")\n",
    "        conv_goal = getattr(conv_out, \"conversation_goal\", \"\")\n",
    "\n",
    "        # 3) reason about pragmatic needs and propose a cooperative query\n",
    "        reasoning = self.reason(conversation_history=conversation_history,\n",
    "                                current_question=question,\n",
    "                                retrieved_context=base_ctx)\n",
    "        pragmatic_needs = getattr(reasoning, \"pragmatic_needs\", \"\")\n",
    "        coop_query = getattr(reasoning, \"cooperative_query\", None) or question\n",
    "\n",
    "        # 4) cooperative re-retrieval\n",
    "        coop_ctx_chunks = self._retrieve(topic, coop_query, k=k)\n",
    "        coop_ctx = self._join_ctx(coop_ctx_chunks)\n",
    "\n",
    "        # 5) synthesize cooperative answer\n",
    "        gen = self.answerer(\n",
    "            question=question,\n",
    "            user_interests=user_interests,\n",
    "            pragmatic_needs=pragmatic_needs,\n",
    "            literal_context=base_ctx,\n",
    "            pragmatic_context=coop_ctx\n",
    "        )\n",
    "        final_text = getattr(gen, \"response\", \"\") or \"\"\n",
    "\n",
    "        # Important for Evaluate: return an object with field `response`\n",
    "        return dspy.Prediction(response=final_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P2.5 - Load PRAGMATICQA data splits\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "DATASET_DIR = \"../PragmatiCQA/data\"  # adjust if needed\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, \"train.jsonl\")\n",
    "\n",
    "# optional tiny train slice for compilation\n",
    "train_data = read_jsonl(TRAIN_PATH)[:50]  # small subset to keep compile fast\n",
    "\n",
    "len(test_data), len(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluating the LLM-Based PragmaticRAG Model__\n",
    "\n",
    "Finally, we evaluate the PragmaticRAG module on the validation set. We consider two cases:\n",
    "\n",
    "* First questions only: Evaluate on the first turn of each conversation (to compare directly with the Part 1 baseline).\n",
    "\n",
    "* All conversation turns: Evaluate on every question in each conversation (with full conversational context).\n",
    "\n",
    "We use the Semantic F1 metric again to judge the quality of answers. The evaluation procedure runs the PragmaticRAG model on each relevant input and computes average Precision, Recall, F1, and Semantic F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2.6 - SemanticF1 evaluation wrapper\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "def evaluate_semantic_f1(examples, decompositional=True):\n",
    "    metric = SemanticF1(decompositional=decompositional)\n",
    "    preds, golds = [], []\n",
    "    for ex in examples:\n",
    "        preds.append(dspy.Example(question=ex[\"question\"], response=ex[\"prediction\"], inputs={\"context\": \"\"}))\n",
    "        golds.append(dspy.Example(question=ex[\"question\"], response=ex[\"reference\"], inputs={\"context\": \"\"}))\n",
    "    batch = metric.batch(preds, golds)\n",
    "\n",
    "    detailed = []\n",
    "    P, R, F = [], [], []\n",
    "    for ex, sc in zip(examples, batch):\n",
    "        p = sc.get(\"precision\", 0.0); r = sc.get(\"recall\", 0.0); f = sc.get(\"f1\", 0.0)\n",
    "        detailed.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"prediction\": ex[\"prediction\"],\n",
    "            \"reference\": ex[\"reference\"],\n",
    "            \"topic\": ex.get(\"topic\",\"\"),\n",
    "            \"precision\": p, \"recall\": r, \"score\": f\n",
    "        })\n",
    "        P.append(p); R.append(r); F.append(f)\n",
    "\n",
    "    n = max(1, len(F))\n",
    "    overall = {\"precision\": sum(P)/n, \"recall\": sum(R)/n, \"f1\": sum(F)/n}\n",
    "    return {\"overall\": overall, \"detailed_results\": detailed}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation on First Questions\n",
    "Evaluate the pragmatic LLM on first-turn questions and compare to the traditional baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/20 23:34:44 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:34:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/20 23:34:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/20 23:35:06 INFO dspy.evaluate.evaluate: Average Metric: 58.73737635224142 / 179 (32.8%)\n",
      "2025/10/20 23:35:06 INFO dspy.evaluate.evaluate: Average Metric: 58.73737635224142 / 179 (32.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1 (First questions) SemanticF1: 32.81\n"
     ]
    }
   ],
   "source": [
    "# 4.4.1 - Build devset for first questions (no history)\n",
    "from dspy.evaluate import Evaluate, SemanticF1\n",
    "\n",
    "def build_devset_first_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        qas = conv.get(\"qas\", [])\n",
    "        if not qas:\n",
    "            continue\n",
    "        q0 = qas[0]\n",
    "        dev.append(\n",
    "            dspy.Example(\n",
    "                topic=conv.get(\"topic\",\"\"),\n",
    "                conversation_history=[],          # no history for first turn\n",
    "                question=q0[\"q\"],\n",
    "                response=q0[\"a\"]                  # gold reference\n",
    "            ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "        )\n",
    "    return dev\n",
    "\n",
    "# cache retrievers once\n",
    "prog_first = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_first = build_devset_first_questions(test_data)\n",
    "evaluator_first = Evaluate(devset=devset_first, metric=SemanticF1(decompositional=True))\n",
    "report_first = evaluator_first(prog_first)\n",
    "print(\"4.4.1 (First questions) SemanticF1:\", report_first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation on All Turns (Conversational Context)\n",
    "Evaluate the model using full conversation context and analyze performance changes across turns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built retrievers for 11/11 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/20 23:46:04 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:46:34 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:46:34 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/20 23:48:33 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:48:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/20 23:53:47 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:56:26 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:56:54 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:56:54 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/20 23:57:06 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/20 23:57:06 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:03:02 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:03:02 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:03:04 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:03:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:04:01 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:04:01 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:04:03 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:04:03 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:10:54 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:24:03 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:24:03 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:40:32 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:40:32 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:47:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:49:09 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:49:09 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:56:31 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:56:31 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 00:56:55 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 00:56:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:01:53 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:01:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:01:59 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:01:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:04:29 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:04:29 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:25 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:25 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:35 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:35 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:47 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:47 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:52 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:52 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:08:56 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:08:56 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:09:04 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:09:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:09:12 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:09:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:10:00 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:10:00 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:11:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:11:43 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:19:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:19:23 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:29:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:29:19 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:29:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:29:24 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:29:36 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:29:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:29:40 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:29:40 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:33:03 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:33:03 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:33:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:33:24 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:35:30 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:35:30 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:35:56 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:35:56 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:36:58 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:36:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:37:10 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:37:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:37:13 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:37:13 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:39:34 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:39:34 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:39:51 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:39:51 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:43:59 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:43:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:44:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:44:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:44:23 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:44:25 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:44:25 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:48:50 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:50:37 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:51:54 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:51:54 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:52:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:52:24 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:56:21 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:56:21 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:56:51 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:56:51 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 01:56:58 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 01:56:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:00:10 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:00:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:02:30 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:02:30 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:02:48 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:02:48 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:02:59 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:02:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:04:24 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:04:24 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:05:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:05:23 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:05:32 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:05:32 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:05:35 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:05:35 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:05:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:05:42 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:05:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:05:42 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:06:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:06:23 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:06:50 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:06:50 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:06:55 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:06:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:06:57 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:06:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:07:04 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/10/21 02:07:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/10/21 02:07:46 INFO dspy.evaluate.evaluate: Average Metric: 470.98502916266955 / 1526 (30.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.2 (All turns) SemanticF1: 30.86\n"
     ]
    }
   ],
   "source": [
    "# 4.4.2 - Build devset for all questions with rolling history\n",
    "def build_devset_all_questions(conversations):\n",
    "    dev = []\n",
    "    for conv in conversations:\n",
    "        topic = conv.get(\"topic\",\"\")\n",
    "        hist = []\n",
    "        for qa in conv.get(\"qas\", []):\n",
    "            dev.append(\n",
    "                dspy.Example(\n",
    "                    topic=topic,\n",
    "                    conversation_history=list(hist),   # copy history so far\n",
    "                    question=qa[\"q\"],\n",
    "                    response=qa[\"a\"]\n",
    "                ).with_inputs(\"topic\", \"conversation_history\", \"question\")\n",
    "            )\n",
    "            hist.append((qa[\"q\"], qa[\"a\"]))  # update rolling history\n",
    "    return dev\n",
    "\n",
    "retr_lookup_val = cache_topic_retrievers(test_data, k=5)\n",
    "prog_all = PragmaticRAG(retriever_lookup=retr_lookup_val, default_k=5)\n",
    "\n",
    "devset_all = build_devset_all_questions(test_data)\n",
    "evaluator_all = Evaluate(devset=devset_all, metric=SemanticF1(decompositional=True))\n",
    "report_all = evaluator_all(prog_all)\n",
    "print(\"4.4.2 (All turns) SemanticF1:\", report_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Optimization Metric:**  \n",
    "> All DSPy evaluations (4.4.1 and 4.4.2) are optimized using the **Semantic F1** metric, which measures conveyed meaning overlap rather than surface token similarity. This ensures that the model is rewarded for semantically accurate and pragmatically coherent answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to part2_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save all evaluation results to JSON ---\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": \"PragmaticRAG\",\n",
    "    \"dataset\": \"PragmatiCQA\",\n",
    "    \"results\": {\n",
    "        \"part_4_4_1_first_questions\": {\n",
    "            \"semantic_f1\": str(report_first)\n",
    "        },\n",
    "        \"part_4_4_2_all_turns\": {\n",
    "            \"semantic_f1\": str(report_all)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "OUTPUT_FILE = \"part2_results.json\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-Based PragmaticRAG Results and Discussion\n",
    "\n",
    "The **LLM-based PragmaticRAG** model achieved a **Semantic F1 of 32.81 %** on first-turn questions (4.4.1) and **30.86 %** when evaluated on all conversation turns (4.4.2).  \n",
    "This means the model performs slightly better on isolated first questions, while longer multi-turn context introduces mild degradation – likely due to accumulated noise, topic drift, and the difficulty of maintaining focus across extended dialogue.  \n",
    "Nevertheless, the LLM still **outperforms the traditional DistilBERT baseline** in the realistic retrieved-context setting (~0.31 vs 0.15 Semantic F1), confirming that **pragmatic reasoning and multi-step retrieval** provide tangible benefits in cooperative question answering.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5 – Discussion\n",
    "\n",
    "### 🔹 Comparison of Models\n",
    "\n",
    "This assignment compared two question-answering strategies:\n",
    "\n",
    "- **Traditional QA (DistilBERT baseline):** a literal, extractive QA model applied with three types of context – literal spans, pragmatic spans, and retrieved passages.  \n",
    "- **LLM-based PragmaticRAG:** a multi-step, retrieval-augmented reasoning system (using DSPy) evaluated on both first questions (single-turn, Section 4.4.1) and full conversations (multi-turn, Section 4.4.2).\n",
    "\n",
    "---\n",
    "\n",
    "### Quantitative Summary\n",
    "\n",
    "| Model / Context | Precision | Recall | F1 | Semantic F1 |\n",
    "|:----------------|:----------:|:------:|:--:|:------------:|\n",
    "| **Baseline Literal** | 0.522 | 0.074 | 0.118 | **0.424** |\n",
    "| **Baseline Pragmatic** | 0.563 | 0.091 | 0.143 | **0.383** |\n",
    "| **Baseline Retrieved** | 0.230 | 0.039 | 0.062 | **0.149** |\n",
    "| **PragmaticRAG (First Q)** | — | — | — | **0.33** |\n",
    "| **PragmaticRAG (All Qs)** | — | — | — | **0.31** |\n",
    "\n",
    "*(Precision / Recall / F1 for the LLM model are not directly comparable, so we focus on Semantic F1.)*\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The **DistilBERT baseline** achieved high precision but extremely low recall, indicating it often extracts only a small part of the full answer.  \n",
    "With literal context, it may capture one fact correctly (precision ≈ 0.52) but miss many details (recall ≈ 0.07).  \n",
    "Adding **pragmatic spans** increases recall slightly (≈ 0.09), showing that cooperative context helps even a simple extractive model.  \n",
    "However, using **retrieved context** without oracle spans causes a steep drop (Semantic F1 ≈ 0.15), proving that naïve retrieval often returns incomplete or noisy text.\n",
    "\n",
    "By contrast, the **LLM-based PragmaticRAG** achieves substantially higher Semantic F1 despite not optimizing for token overlap.  \n",
    "Its answers are longer, context-aware, and conversationally rich.  \n",
    "On first-turn questions, PragmaticRAG’s Semantic F1 (0.33) slightly exceeds its multi-turn score (0.31), showing that the model answers individual questions more accurately than extended dialogues.  \n",
    "The small (~2-point) drop reflects the added difficulty of handling long conversational history.  \n",
    "Still, the model’s later-turn answers remain coherent and often integrate previous context – something the baseline rarely manages.\n",
    "\n",
    "**Qualitatively, PragmaticRAG can:**\n",
    "- Clarify user confusions (e.g., interpreting “hop hop music” as “hip-hop”).  \n",
    "- Elaborate on topics with relevant background.  \n",
    "- Maintain conversational continuity across turns.  \n",
    "\n",
    "Overall, **PragmaticRAG demonstrates that pragmatic reasoning and multi-step retrieval enhance response quality**, even if long-context reasoning remains mildly challenging.\n",
    "\n",
    "---\n",
    "\n",
    "### Theory of Mind and Pragmatic Reasoning\n",
    "\n",
    "The model shows partial *Theory of Mind*-like behavior through pragmatic competencies that mimic perspective-taking:\n",
    "\n",
    "- **Intent recognition:** detects implicit questions or typos and clarifies them.  \n",
    "- **Perspective-taking:** conditions responses on prior dialogue to avoid redundancy.  \n",
    "- **Anticipation of needs:** occasionally adds context or offers follow-ups, echoing cooperative human conversation.\n",
    "\n",
    "These patterns emerge from large-scale conversational training and prompt design rather than genuine mental-state reasoning.  \n",
    "In essence, the model statistically **imitates Theory-of-Mind-like behavior**, which is precisely what PragmatiCQA aims to evaluate.\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever and Metric Notes\n",
    "\n",
    "- **Retrieval Quality:** our FAISS retriever (k = 5) performed adequately but inconsistently. Topics with well-structured documents (e.g., *Hamilton the Musical*) produced stronger answers. Increasing *k* or using a higher-quality embedding model could raise overall scores.  \n",
    "- **Evaluation Metric:** **Semantic F1** (LLM-based semantic overlap) captures meaning similarity rather than exact wording – ideal for pragmatic free-form answers.  \n",
    "- **Decompositional Scoring:** enabling `decompositional=True` grants partial credit for partially correct sub-answers, slightly lifting absolute values without altering trends.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "| Aspect | Traditional QA Baseline | LLM PragmaticRAG (Our Approach) |\n",
    "|:-------|:------------------------|:--------------------------------|\n",
    "| **Context Use** | Single-turn (no history) | Full conversation + retrieved docs |\n",
    "| **Reasoning** | Span extraction | Multi-step reasoning (Chain-of-Thought) |\n",
    "| **Strengths** | Exact fact precision; fast | Coherence, informativeness, intent awareness |\n",
    "| **Weaknesses** | Low recall; no context integration | Slight context drift in long dialogs |\n",
    "| **Typical Semantic F1** | 0.15 (retrieved) → 0.42 (oracle) | 0.31 (multi-turn) → 0.33 (first-turn) |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The **DistilBERT baseline** captures literal correctness but lacks pragmatic depth.  \n",
    "The **LLM PragmaticRAG** model reaches comparable or higher Semantic F1 while demonstrating **human-like cooperative reasoning**.  \n",
    "It enriches answers with clarifications, elaborations, and anticipatory follow-ups – hallmarks of pragmatic and Theory-of-Mind-aligned behavior that the **PragmatiCQA** benchmark is designed to measure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
